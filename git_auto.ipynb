{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<git.repo.base.Repo 'e:\\\\UR\\\\TFG\\\\data\\\\.git'>\n",
      "Changes detected.\n",
      "diff --git a/Linear.ipynb b/Linear.ipynb\n",
      "index 1079345..b9df89f 100644\n",
      "--- a/Linear.ipynb\n",
      "+++ b/Linear.ipynb\n",
      "@@ -15,7 +15,7 @@\n",
      "   },\n",
      "   {\n",
      "    \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 25,\n",
      "+   \"execution_count\": 42,\n",
      "    \"metadata\": {},\n",
      "    \"outputs\": [],\n",
      "    \"source\": [\n",
      "@@ -29,7 +29,7 @@\n",
      "   },\n",
      "   {\n",
      "    \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 13,\n",
      "+   \"execution_count\": 43,\n",
      "    \"metadata\": {},\n",
      "    \"outputs\": [\n",
      "     {\n",
      "@@ -239,29 +239,29 @@\n",
      "        \"4             5.371600             4.608560             5.483689  \"\n",
      "       ]\n",
      "      },\n",
      "-     \"execution_count\": 13,\n",
      "+     \"execution_count\": 43,\n",
      "      \"metadata\": {},\n",
      "      \"output_type\": \"execute_result\"\n",
      "     }\n",
      "    ],\n",
      "    \"source\": [\n",
      "-    \"df = pd.read_csv(r'./preprocess.csv')\\n\",\n",
      "+    \"df = pd.read_csv(r'./data/preprocess.csv')\\n\",\n",
      "     \"\\n\",\n",
      "     \"df.head()\"\n",
      "    ]\n",
      "   },\n",
      "   {\n",
      "    \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 14,\n",
      "+   \"execution_count\": 44,\n",
      "    \"metadata\": {},\n",
      "    \"outputs\": [],\n",
      "    \"source\": [\n",
      "-    \"target = pd.read_csv('target_PROD.csv')\"\n",
      "+    \"target = pd.read_csv('./data/target_PROD.csv')\"\n",
      "    ]\n",
      "   },\n",
      "   {\n",
      "    \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 15,\n",
      "+   \"execution_count\": 45,\n",
      "    \"metadata\": {},\n",
      "    \"outputs\": [\n",
      "     {\n",
      "@@ -270,7 +270,7 @@\n",
      "        \"'[31595, 3285, 2581, 13155, 2487, 0, 3843, 2537, 14912, 10374, 5482, 6076, 4489, 1235, 12240, 10599, 4426, 691, 5631, 2173, 1729, 12134, 3248, 4940, 4322]'\"\n",
      "       ]\n",
      "      },\n",
      "-     \"execution_count\": 15,\n",
      "+     \"execution_count\": 45,\n",
      "      \"metadata\": {},\n",
      "      \"output_type\": \"execute_result\"\n",
      "     }\n",
      "diff --git a/datos_df3.csv b/datos_df3.csv\n",
      "deleted file mode 100644\n",
      "index 76983bf..0000000\n",
      "--- a/datos_df3.csv\n",
      "+++ /dev/null\n",
      "@@ -1,26 +0,0 @@\n",
      "-IDX,mean_ndvi_2022-06-02,mean_ndvi_2022-06-17,mean_ndvi_2022-06-07,mean_ndvi_2022-06-27,mean_ndvi_2022-06-22,mean_ndvi_2022-06-12,mean_ndre_2022-06-02,mean_ndre_2022-06-17,mean_ndre_2022-06-07,mean_ndre_2022-06-27,mean_ndre_2022-06-22,mean_ndre_2022-06-12,IDX,sum_ndvi_2022-06-02,sum_ndvi_2022-06-17,sum_ndvi_2022-06-07,sum_ndvi_2022-06-27,sum_ndvi_2022-06-22,sum_ndvi_2022-06-12,sum_ndre_2022-06-02,sum_ndre_2022-06-17,sum_ndre_2022-06-07,sum_ndre_2022-06-27,sum_ndre_2022-06-22,sum_ndre_2022-06-12\n",
      "-3942,0.007476411960018109,0.2127703350866087,0.18084413042287567,-0.0077050013056220775,0.03847764968411478,0.21989588925884906,-0.017072965787758528,0.1517137726781239,0.1237459981978032,-0.027110648241502125,-0.0036537221342088978,0.15333288888355717,3942,0.5308252491612857,15.106693791149217,12.839933260024173,-0.5470550926991675,2.7319131275721493,15.612608137378283,-1.2121805709308555,10.771677860146795,8.785965872044027,-1.9248560251466509,-0.25941427152883173,10.88663511073256\n",
      "-4053,0.008708933354730414,0.2020584995264399,0.1650456686788255,-0.0054598499614404265,-0.019583986693047915,0.1988123692947914,-0.007737753379226725,0.13488629075599046,0.1060430193136325,-0.02560667459002825,-0.039130129152840705,0.1287979661240226,4053,1.6895330708177,39.19934890812934,32.018859723692145,-1.0592108925194428,-3.7992934184512954,38.569599643189534,-1.5011241555699846,26.167940406662147,20.572345746844704,-4.967694870465481,-7.591245055651097,24.986805428060386\n",
      "-4196,0.20457577926079018,0.1929386520555124,0.15674524178876795,0.001064647090296338,-0.011448087035602933,0.1866423150688147,0.22634690532533708,0.12865776242585927,0.09565639535042085,-0.023633105798599074,-0.04508807153855535,0.11709020264342655,4196,19.639274809035857,18.52211059732919,15.047543211721724,0.10220612066844846,-1.0990163554178816,17.917662246606213,21.72930291123236,12.35114519288249,9.183013953640401,-2.268778156665511,-4.328454867701313,11.240659453768949\n",
      "-4422,-0.001837647991757089,0.198716591902953,0.1813427211269491,-0.009038437144779943,-0.002893956806626177,0.21542823393981722,-0.036877994154296954,0.132829284745391,0.11387893158684305,-0.028160211389675657,-0.029515738598704908,0.13608861202434913,4422,-0.10474593553015407,11.32684573846832,10.336535104236098,-0.5151909172524568,-0.16495553797769208,12.279409334569582,-2.1020456667949263,7.571269230487287,6.491099100450054,-1.6051320492115124,-1.6823971001261797,7.7570508853879\n",
      "-4424,-0.04676209283678191,0.20548152280363527,0.18570757721097278,-0.01025040966809242,-0.005376541780280527,0.21823700563116044,-0.0942551479805548,0.14517838185831325,0.12455566838931278,-0.0288221479983803,-0.031291626093822744,0.14820779875859866,4424,-1.7301974349609308,7.602816343734505,6.871180356805993,-0.37926515771941954,-0.1989320458703795,8.074769208352937,-3.4874404752805277,5.37160012875759,4.608559730404573,-1.0664194759400711,-1.1577901654714415,5.48368855406815\n",
      "-3882,0.0005918491361654385,0.22486520946635452,0.18962633047632066,-0.004973985796906293,0.062245579007965464,0.23351470992835316,-0.018171755408306685,0.1592563116713329,0.12956089561567358,-0.02439197137948016,0.04125490948033066,0.15971539548786778,3882,0.033143551625264556,12.592451730115853,10.619074506673957,-0.2785432046267524,3.485752424446066,13.076823755987776,-1.0176183028651744,8.918353453594642,7.255410154477721,-1.365950397250889,2.310274930898517,8.944062147320595\n",
      "-4732,0.25624543676785344,0.1777468812217337,0.14113332507352483,-0.0006393567424101583,-0.05428090919844213,0.1700758344283176,0.25211035856309916,0.11416316860088876,0.0808824621679879,-0.03074896959181561,-0.10370772250174883,0.10190735323497953,4732,9.224835723642723,6.398887723982414,5.080799702646894,-0.0230168427267657,-1.9541127311439168,6.122730039419434,9.07597290827157,4.109874069631996,2.9117686380475645,-1.106962905305362,-3.733478010062958,3.668664716459263\n",
      "-3553,0.036758823365495824,0.1198552052130903,0.11641151858847808,-0.006665990705653675,-0.0026436970959803573,0.10655929035833445,0.028543868072022374,0.07620794761395483,0.07095270277393423,-0.026473855799232084,-0.03169789567572757,0.05999168918217972,3553,2.756911752412187,8.989140390981772,8.730863894135856,-0.4999493029240257,-0.1982772821985268,7.991946776875084,2.140790105401678,5.715596071046612,5.321452708045067,-1.9855391849424062,-2.377342175679568,4.499376688663479\n",
      "-3582,0.1156269993239312,0.21608109155552557,0.1763527478857893,0.004865472800519855,-0.002794858304314505,0.2092272416387304,0.0915893019374318,0.14404622803453848,0.11056993666165864,-0.022402475136793044,-0.035854478218951796,0.13156873062070595,3582,6.590738961464078,12.316622218664957,10.052106629489991,0.27733194962963176,-0.15930692334592678,11.925952773407634,5.220590210433612,8.210634997968693,6.3024863897145424,-1.2769410827972034,-2.0437052584802524,7.499417645380239\n",
      "-3914,0.17255847096102025,0.31594087295828,0.2916764013118253,0.005313633147867869,-0.02869033353707477,0.3645014865397951,0.1278668869317516,0.21020454603949063,0.20028127504484164,-0.022136758932456032,-0.06350669740363223,0.2433468893843765,3914,22.260042753971614,40.75637261161812,37.626255769225466,0.685458676074955,-3.7010530262826453,47.020691763633565,16.494828414195958,27.116386439094292,25.83628448078457,-2.8556419022868282,-8.192363965068557,31.39174873058457\n",
      "-3998,-0.04862021505382013,0.2271254970774861,0.20371515705462542,-0.006980316642397973,0.011323331323264484,0.2536537816786206,-0.0827527011734857,0.15148280899371988,0.13274439027749665,-0.027303658726231982,-0.021211192673721433,0.16339118562022603,3998,-2.8199724731215676,13.173278830494194,11.815479109168274,-0.4048583652590824,0.6567532167493401,14.711919337359996,-4.799656668062171,8.786002921635752,7.699174636094806,-1.583612206121455,-1.230249175075843,9.47668876597311\n",
      "-4156,-0.02258213756790741,0.20601049780230646,0.17768448649916185,-0.004524013349604875,0.016386303826855003,0.21570598981921552,-0.05936319751825201,0.14606744135072042,0.1222188411247505,-0.024723870544866713,-0.011739110629751966,0.14703780934363928,4156,-2.5066172700377227,22.867165256056015,19.722978001406965,-0.5021654818061411,1.8188797247809052,23.94336486993292,-6.589314924525973,16.213485989929968,13.566291364847306,-2.744349630480205,-1.3030412799024682,16.321196837143958\n",
      "-4245,0.002301049317138844,0.19282554445729927,0.18496826326599652,-0.006524370152732699,0.0155281106495753,0.21315120157443188,-0.03906032443791141,0.13922784970023636,0.1344424764647228,-0.02767662353468433,-0.007468187496824657,0.15358259052037515,4245,0.09894512063697031,8.291498411663868,7.953635320437851,-0.28054791656750605,0.6677087579317379,9.165501667700571,-1.6795939508301907,5.986797537110164,5.78102648798308,-1.190094811991426,-0.32113206236346026,6.604051392376132\n",
      "-4261,0.00017252770774520997,0.1814643467980725,0.16448851496785324,-0.0008037201379987014,0.012327207877943315,0.18284318900350624,-0.014316107690479165,0.11752121748005943,0.10649432389319656,-0.020159957232437138,-0.022746566592127013,0.11523435042171938,4261,0.035885763211003674,37.744584133999076,34.213611113313476,-0.1671737887037299,2.5640592386122094,38.0313833127293,-2.977750399619666,24.444413235852362,22.150819369784884,-4.193271104346925,-4.7312858511624185,23.96874488771763\n",
      "-4263,0.13232465620041967,0.2218479000936428,0.21491711157710838,-0.008402525556153864,0.03263829989344912,0.2618118832222543,0.14876100080768062,0.13977789184249997,0.1435285260708006,-0.02633033526842834,0.005760696876455521,0.17048846894532166,4263,13.232465620041967,22.18479000936428,21.49171115771084,-0.8402525556153864,3.263829989344912,26.18118832222543,14.876100080768062,13.977789184249998,14.352852607080061,-2.633033526842834,0.5760696876455521,17.048846894532165\n",
      "-5246,0.005992455902897741,0.21413910427840468,0.20262568532568279,-0.00702591191854299,-0.012471227311634456,0.23221537110749277,-0.005717275569861807,0.1488327594134763,0.13791840950564732,-0.026320487571836398,-0.04620476505870574,0.15567307173783382,5246,0.4973738399405125,17.77354565510759,16.817931882031672,-0.5831506892390682,-1.0351118668656598,19.2738758019219,-0.47453387229852995,12.353119031318533,11.447227988968727,-2.184600468462421,-3.834995499872577,12.920864954240207\n",
      "-3502,0.18518894381936918,0.21813016895605017,0.187833795358135,-0.0019677618551283884,-0.03185412671820722,0.22908608964239277,0.22432550681194333,0.14376316744140047,0.12037654995620813,-0.027777718137988586,-0.06431077342264328,0.14732852750939915,3502,46.29723595484229,54.53254223901254,46.958448839533744,-0.49194046378209705,-7.963531679551805,57.271522410598195,56.08137670298583,35.94079186035012,30.09413748905203,-6.9444295344971465,-16.07769335566082,36.83213187734979\n",
      "-4285,0.1507574438860988,0.25660632973425923,0.21372754308463215,-0.001232139280400841,-0.008045909588246924,0.26201608461295267,0.16903258006141442,0.17681730513063088,0.13621850991036674,-0.026740467923785104,-0.04268363912150829,0.169355531184839,4285,18.99543792964845,32.33239754651666,26.929670428663652,-0.15524954933050597,-1.0137846081191124,33.01402666123204,21.298105087738218,22.27898044645949,17.16353224870621,-3.3692989583969233,-5.3781385293100445,21.338796929289714\n",
      "-4777,0.02604618152101297,0.22568567422896424,0.1912748446669856,-0.0069356922059052644,0.0431576574815238,0.24406468230675915,-0.0043803022764667306,0.15135793361095257,0.12659551147139855,-0.02784364428418655,0.009328243039582788,0.16117073922844743,4777,2.682756696664336,23.245624445583317,19.701309000699517,-0.7143762972082423,4.445238720596952,25.138662277596193,-0.45117113447607327,15.589867161928115,13.03933768155405,-2.8678953612712146,0.9608090330770271,16.600586140530087\n",
      "-3804,0.009015554545493153,0.2249113677422857,0.21074392899712302,-0.009843359512821791,0.01882039240115871,0.23347236103133667,-0.012431895084969022,0.15207887437979853,0.1382932256854848,-0.030709237559030093,-0.0005110094822630468,0.15244509315990942,3804,2.17274864546385,54.20363962589086,50.78928688830665,-2.372249642590052,4.535714568679249,56.26683900855214,-2.996086715477534,36.65100872553145,33.328667390201836,-7.400926251726252,-0.12315328522539426,36.73926745153817\n",
      "-4000,0.15495454784074383,0.22645370516711408,0.19904234486499536,0.003923775279872905,-0.0026035611941514123,0.23680623598499512,0.12203867808404427,0.1504950905829815,0.12819492901764165,-0.021984986434348933,-0.031176662658276153,0.15347424962839074,4000,40.13322789075265,58.65150963828255,51.551967320033796,1.0162577974870826,-0.6743223492852157,61.332815120113736,31.608017623767466,38.978228460992206,33.202486615569185,-5.694111486496373,-8.074755628493524,39.7498306537532\n",
      "-4189,-0.015729714299690188,0.2195371700455565,0.19062692674660073,-0.009184057560900748,0.012396695002354793,0.23387601243691172,-0.04436212580041925,0.15409390164730985,0.12252633522701059,-0.029183813044626392,-0.014482850670797036,0.15436408576017707,4189,-2.0291331446600345,28.320294935876788,24.590873550311493,-1.1847434253561966,1.5991736553037683,30.170005604361613,-5.722714228254083,19.878113312502972,15.805897244284367,-3.7647118827568047,-1.8682877365328177,19.912967063062844\n",
      "-4262,0.1883804343832237,0.24999373582913204,0.22496563139002138,-0.00770426238010893,0.03488284367192087,0.2787068496062342,0.2254958615870058,0.16826002552983477,0.15532658726093296,-0.026807633392945417,0.018039677712027278,0.1909229310532905,4262,36.73418470472862,48.74877848668075,43.86829812105417,-1.5023311641212413,6.8021545160245696,54.34783567321567,43.97169300946613,32.81070497831778,30.28868451588193,-5.227488511624356,3.517737153845319,37.22997155539165\n",
      "-4284,0.028524316549603284,0.22981639681541277,0.19872239251901275,-0.00599662629184433,0.011166697460329495,0.23695085622061293,0.004692073453361286,0.1660903787116705,0.13605286948319156,-0.028129064781672698,0.02324918708506416,0.16226920830768993,4284,0.5704863309920657,4.596327936308255,3.9744478503802547,-0.11993252583688659,0.2233339492065899,4.739017124412259,0.09384146906722571,3.3218075742334103,2.721057389663831,-0.5625812956334539,0.4649837417012832,3.2453841661537988\n",
      "-4426,-0.009749057109967277,0.22482554589856543,0.21339360443954444,-0.007608461011219226,0.0006529451694381423,0.25152827200850025,-0.04909511836538409,0.1502989235217334,0.14066988517477524,-0.027662853780128095,-0.023714145717748006,0.16143818108956134,4426,-2.1447925641928007,49.461620097684396,46.946592976699776,-1.6738614224682296,0.1436479372763913,55.33621984187006,-10.8009260403845,33.06576317478135,30.947374738450556,-6.085827831628181,-5.217112057904561,35.516399839703496\n",
      "diff --git a/datos_df3.txt b/datos_df3.txt\n",
      "deleted file mode 100644\n",
      "index 61c8c3a..0000000\n",
      "--- a/datos_df3.txt\n",
      "+++ /dev/null\n",
      "@@ -1,26 +0,0 @@\n",
      "-,mean_ndvi_2023-01-08,mean_ndvi_2023-01-03,mean_ndre_2023-01-08,mean_ndre_2023-01-03,sum_ndvi_2023-01-08,sum_ndvi_2023-01-03,sum_ndre_2023-01-08,sum_ndre_2023-01-03\n",
      "-0,0.004620153984221587,0.15145551067188692,-0.016162110482990415,0.08661784301163361,0.004620153984221587,0.15145551067188692,-0.016162110482990415,0.08661784301163361\n",
      "-1,0.0045369861247231144,0.12724780449162643,-0.017438109007068287,0.074093590644238,0.0045369861247231144,0.12724780449162643,-0.017438109007068287,0.074093590644238\n",
      "-2,0.016170286676439806,0.11784584472538007,0.007062075920191456,0.06987767916546582,0.016170286676439806,0.11784584472538007,0.007062075920191456,0.06987767916546582\n",
      "-3,-0.0001458729230495337,0.1341924794974601,-0.023897930231775912,0.08132003817364962,-0.0001458729230495337,0.1341924794974601,-0.023897930231775912,0.08132003817364962\n",
      "-4,-0.00017290243430005822,0.13523209888104948,-0.023880806017876774,0.08563593337069952,-0.00017290243430005822,0.13523209888104948,-0.023880806017876774,0.08563593337069952\n",
      "-5,0.0016123131612655838,0.16262420167048885,-0.020174141598595294,0.08947936224862592,0.0016123131612655838,0.16262420167048885,-0.020174141598595294,0.08947936224862592\n",
      "-6,0.008360003104184896,0.1420530930190325,-0.0013468848406938657,0.07459869605296393,0.008360003104184896,0.1420530930190325,-0.0013468848406938657,0.07459869605296393\n",
      "-7,0.004323226540260737,0.12116634823858022,-0.017864353196366752,0.07197753115900403,0.004323226540260737,0.12116634823858022,-0.017864353196366752,0.07197753115900403\n",
      "-8,0.010802065893629701,0.14817249083337092,0.001733612921301707,0.07720537145823665,0.010802065893629701,0.14817249083337092,0.001733612921301707,0.07720537145823665\n",
      "-9,0.00801496483913282,0.1907170353823949,-0.003305660469203707,0.11750293975154266,0.00801496483913282,0.1907170353823949,-0.003305660469203707,0.11750293975154266\n",
      "-10,0.008404701945525823,0.1749556079431174,-0.013873153191237973,0.09247048966230571,0.008404701945525823,0.1749556079431174,-0.013873153191237973,0.09247048966230571\n",
      "-11,0.005624529960006803,0.17425404270552036,-0.015095469094956593,0.09831950841226567,0.005624529960006803,0.17425404270552036,-0.015095469094956593,0.09831950841226567\n",
      "-12,0.005643529968510541,0.12268883190944864,-0.01658773962738037,0.09661947058912863,0.005643529968510541,0.12268883190944864,-0.01658773962738037,0.09661947058912863\n",
      "-13,0.008080935356864242,0.14479795369896165,-0.012345390468848453,0.07859878902713713,0.008080935356864242,0.14479795369896165,-0.012345390468848453,0.07859878902713713\n",
      "-14,0.008840193943670761,0.15029132959898445,-0.011941783508243448,0.0850646758877791,0.008840193943670761,0.15029132959898445,-0.011941783508243448,0.0850646758877791\n",
      "-15,0.0029195776540090162,0.14669080791897737,-0.01820885552153725,0.08152448178745848,0.0029195776540090162,0.14669080791897737,-0.01820885552153725,0.08152448178745848\n",
      "-16,0.0052871265268839744,0.1451071625983243,-0.012346873524087629,0.0801447950359134,0.0052871265268839744,0.1451071625983243,-0.012346873524087629,0.0801447950359134\n",
      "-17,0.012488856391982979,0.1573117355173421,0.0010056607569770585,0.09375767835213347,0.012488856391982979,0.1573117355173421,0.0010056607569770585,0.09375767835213347\n",
      "-18,0.0015815409043404098,0.17338377589989076,-0.021245156900505983,0.08902367788337154,0.0015815409043404098,0.17338377589989076,-0.021245156900505983,0.08902367788337154\n",
      "-19,0.006480275334776319,0.1528657497965628,-0.014755225335485886,0.09084951526083843,0.006480275334776319,0.1528657497965628,-0.014755225335485886,0.09084951526083843\n",
      "-20,0.0030681103904122713,0.1520143570977018,-0.01231906448176468,0.08037458033214924,0.0030681103904122713,0.1520143570977018,-0.01231906448176468,0.08037458033214924\n",
      "-21,0.006889065339770724,0.13459827769961796,-0.01479918995991021,0.0798027562015304,0.006889065339770724,0.13459827769961796,-0.01479918995991021,0.0798027562015304\n",
      "-22,0.008631515785139942,0.15940970339894223,-0.013356212466405304,0.10307368377437028,0.008631515785139942,0.15940970339894223,-0.013356212466405304,0.10307368377437028\n",
      "-23,0.006319426551786145,0.11673803656863417,-0.0168629100721563,0.08196776991806247,0.006319426551786145,0.11673803656863417,-0.0168629100721563,0.08196776991806247\n",
      "-24,7.091206995738659e-05,0.1328101218913874,-0.024533381947334187,0.07825303166180339,7.091206995738659e-05,0.1328101218913874,-0.024533381947334187,0.07825303166180339\n",
      "diff --git a/git_auto.ipynb b/git_auto.ipynb\n",
      "index e69de29..76191f1 100644\n",
      "--- a/git_auto.ipynb\n",
      "+++ b/git_auto.ipynb\n",
      "@@ -0,0 +1,68 @@\n",
      "+{\n",
      "+ \"cells\": [\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": null,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [],\n",
      "+   \"source\": []\n",
      "+  },\n",
      "+  {\n",
      "+   \"cell_type\": \"code\",\n",
      "+   \"execution_count\": 1,\n",
      "+   \"metadata\": {},\n",
      "+   \"outputs\": [\n",
      "+    {\n",
      "+     \"name\": \"stdout\",\n",
      "+     \"output_type\": \"stream\",\n",
      "+     \"text\": [\n",
      "+      \"e:\\\\UR\\\\TFG\\\\data\\\\publish\\n\"\n",
      "+     ]\n",
      "+    },\n",
      "+    {\n",
      "+     \"ename\": \"NoSuchPathError\",\n",
      "+     \"evalue\": \"e:\\\\UR\\\\TFG\\\\data\\\\publish\",\n",
      "+     \"output_type\": \"error\",\n",
      "+     \"traceback\": [\n",
      "+      \"\\u001b[1;31m---------------------------------------------------------------------------\\u001b[0m\",\n",
      "+      \"\\u001b[1;31mNoSuchPathError\\u001b[0m                           Traceback (most recent call last)\",\n",
      "+      \"\\u001b[1;32me:\\\\UR\\\\TFG\\\\data\\\\git_auto.ipynb Cell 2\\u001b[0m in \\u001b[0;36m<cell line: 1>\\u001b[1;34m()\\u001b[0m\\n\\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/UR/TFG/data/git_auto.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\\u001b[0m exec(\\u001b[39mopen\\u001b[39;49m(\\u001b[39m\\\"\\u001b[39;49m\\u001b[39mgitpython.py\\u001b[39;49m\\u001b[39m\\\"\\u001b[39;49m)\\u001b[39m.\\u001b[39;49mread())\\n\",\n",
      "+      \"File \\u001b[1;32m<string>:22\\u001b[0m, in \\u001b[0;36m<module>\\u001b[1;34m\\u001b[0m\\n\",\n",
      "+      \"File \\u001b[1;32mc:\\\\Python38\\\\lib\\\\site-packages\\\\git\\\\repo\\\\base.py:152\\u001b[0m, in \\u001b[0;36mRepo.__init__\\u001b[1;34m(self, path, odbt, search_parent_directories, expand_vars)\\u001b[0m\\n\\u001b[0;32m    150\\u001b[0m \\u001b[39mif\\u001b[39;00m epath \\u001b[39mis\\u001b[39;00m \\u001b[39mnot\\u001b[39;00m \\u001b[39mNone\\u001b[39;00m:\\n\\u001b[0;32m    151\\u001b[0m     \\u001b[39mif\\u001b[39;00m \\u001b[39mnot\\u001b[39;00m os\\u001b[39m.\\u001b[39mpath\\u001b[39m.\\u001b[39mexists(epath):\\n\\u001b[1;32m--> 152\\u001b[0m         \\u001b[39mraise\\u001b[39;00m NoSuchPathError(epath)\\n\\u001b[0;32m    154\\u001b[0m \\u001b[39m## Walk up the path to find the `.git` dir.\\u001b[39;00m\\n\\u001b[0;32m    155\\u001b[0m \\u001b[39m#\\u001b[39;00m\\n\\u001b[0;32m    156\\u001b[0m curpath \\u001b[39m=\\u001b[39m epath\\n\",\n",
      "+      \"\\u001b[1;31mNoSuchPathError\\u001b[0m: e:\\\\UR\\\\TFG\\\\data\\\\publish\"\n",
      "+     ]\n",
      "+    }\n",
      "+   ],\n",
      "+   \"source\": [\n",
      "+    \"exec(open(\\\"gitpython.py\\\").read())\"\n",
      "+   ]\n",
      "+  }\n",
      "+ ],\n",
      "+ \"metadata\": {\n",
      "+  \"kernelspec\": {\n",
      "+   \"display_name\": \"Python 3\",\n",
      "+   \"language\": \"python\",\n",
      "+   \"name\": \"python3\"\n",
      "+  },\n",
      "+  \"language_info\": {\n",
      "+   \"codemirror_mode\": {\n",
      "+    \"name\": \"ipython\",\n",
      "+    \"version\": 3\n",
      "+   },\n",
      "+   \"file_extension\": \".py\",\n",
      "+   \"mimetype\": \"text/x-python\",\n",
      "+   \"name\": \"python\",\n",
      "+   \"nbconvert_exporter\": \"python\",\n",
      "+   \"pygments_lexer\": \"ipython3\",\n",
      "+   \"version\": \"3.8.10\"\n",
      "+  },\n",
      "+  \"orig_nbformat\": 4,\n",
      "+  \"vscode\": {\n",
      "+   \"interpreter\": {\n",
      "+    \"hash\": \"9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2\"\n",
      "+   }\n",
      "+  }\n",
      "+ },\n",
      "+ \"nbformat\": 4,\n",
      "+ \"nbformat_minor\": 2\n",
      "+}\n",
      "diff --git a/gitpython.py b/gitpython.py\n",
      "index ea20c34..bf2f60b 100644\n",
      "--- a/gitpython.py\n",
      "+++ b/gitpython.py\n",
      "@@ -15,9 +15,7 @@ NAME = \"alesteba\"\n",
      " \n",
      " path = os.getcwd()\n",
      " \n",
      "-publish_path = os.path.join(path, 'publish')\n",
      "-\n",
      "-print(publish_path)\n",
      "+publish_path = path\n",
      " \n",
      " repo = Repo(publish_path)\n",
      " \n",
      "diff --git a/marcos_geo.ipynb b/marcos_geo.ipynb\n",
      "deleted file mode 100644\n",
      "index 231cffd..0000000\n",
      "--- a/marcos_geo.ipynb\n",
      "+++ /dev/null\n",
      "@@ -1,2050 +0,0 @@\n",
      "-{\n",
      "- \"cells\": [\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": null,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"# prueba de tood el algoritmo y de la implementación:\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 14,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"import pandas as pd\\n\",\n",
      "-    \"import numpy as np\\n\",\n",
      "-    \"import random\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 52,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"import geopandas as gpd\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"pp = gpd.read_file(r'C:\\\\Users\\\\reuni\\\\Desktop\\\\Agrai-World\\\\agrai\\\\data\\\\parcelas\\\\25\\\\parcelas.shp')\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 15,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [\n",
      "-    {\n",
      "-     \"data\": {\n",
      "-      \"text/html\": [\n",
      "-       \"<div>\\n\",\n",
      "-       \"<style scoped>\\n\",\n",
      "-       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
      "-       \"        vertical-align: middle;\\n\",\n",
      "-       \"    }\\n\",\n",
      "-       \"\\n\",\n",
      "-       \"    .dataframe tbody tr th {\\n\",\n",
      "-       \"        vertical-align: top;\\n\",\n",
      "-       \"    }\\n\",\n",
      "-       \"\\n\",\n",
      "-       \"    .dataframe thead th {\\n\",\n",
      "-       \"        text-align: right;\\n\",\n",
      "-       \"    }\\n\",\n",
      "-       \"</style>\\n\",\n",
      "-       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
      "-       \"  <thead>\\n\",\n",
      "-       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
      "-       \"      <th></th>\\n\",\n",
      "-       \"      <th>IDX</th>\\n\",\n",
      "-       \"      <th>mean_ndvi_2022-06-02</th>\\n\",\n",
      "-       \"      <th>mean_ndvi_2022-06-17</th>\\n\",\n",
      "-       \"      <th>mean_ndvi_2022-06-07</th>\\n\",\n",
      "-       \"      <th>mean_ndvi_2022-06-22</th>\\n\",\n",
      "-       \"      <th>mean_ndvi_2022-06-12</th>\\n\",\n",
      "-       \"      <th>mean_ndre_2022-06-02</th>\\n\",\n",
      "-       \"      <th>mean_ndre_2022-06-17</th>\\n\",\n",
      "-       \"      <th>mean_ndre_2022-06-07</th>\\n\",\n",
      "-       \"      <th>mean_ndre_2022-06-12</th>\\n\",\n",
      "-       \"      <th>IDX.1</th>\\n\",\n",
      "-       \"      <th>sum_ndvi_2022-06-02</th>\\n\",\n",
      "-       \"      <th>sum_ndvi_2022-06-17</th>\\n\",\n",
      "-       \"      <th>sum_ndvi_2022-06-07</th>\\n\",\n",
      "-       \"      <th>sum_ndvi_2022-06-22</th>\\n\",\n",
      "-       \"      <th>sum_ndvi_2022-06-12</th>\\n\",\n",
      "-       \"      <th>sum_ndre_2022-06-02</th>\\n\",\n",
      "-       \"      <th>sum_ndre_2022-06-17</th>\\n\",\n",
      "-       \"      <th>sum_ndre_2022-06-07</th>\\n\",\n",
      "-       \"      <th>sum_ndre_2022-06-12</th>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"  </thead>\\n\",\n",
      "-       \"  <tbody>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>0</th>\\n\",\n",
      "-       \"      <td>3942</td>\\n\",\n",
      "-       \"      <td>0.007476</td>\\n\",\n",
      "-       \"      <td>0.212770</td>\\n\",\n",
      "-       \"      <td>0.180844</td>\\n\",\n",
      "-       \"      <td>0.038478</td>\\n\",\n",
      "-       \"      <td>0.219896</td>\\n\",\n",
      "-       \"      <td>-0.017073</td>\\n\",\n",
      "-       \"      <td>0.151714</td>\\n\",\n",
      "-       \"      <td>0.123746</td>\\n\",\n",
      "-       \"      <td>0.153333</td>\\n\",\n",
      "-       \"      <td>3942</td>\\n\",\n",
      "-       \"      <td>0.530825</td>\\n\",\n",
      "-       \"      <td>15.106694</td>\\n\",\n",
      "-       \"      <td>12.839933</td>\\n\",\n",
      "-       \"      <td>2.731913</td>\\n\",\n",
      "-       \"      <td>15.612608</td>\\n\",\n",
      "-       \"      <td>-1.212181</td>\\n\",\n",
      "-       \"      <td>10.771678</td>\\n\",\n",
      "-       \"      <td>8.785966</td>\\n\",\n",
      "-       \"      <td>10.886635</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>1</th>\\n\",\n",
      "-       \"      <td>4053</td>\\n\",\n",
      "-       \"      <td>0.008709</td>\\n\",\n",
      "-       \"      <td>0.202058</td>\\n\",\n",
      "-       \"      <td>0.165046</td>\\n\",\n",
      "-       \"      <td>-0.019584</td>\\n\",\n",
      "-       \"      <td>0.198812</td>\\n\",\n",
      "-       \"      <td>-0.007738</td>\\n\",\n",
      "-       \"      <td>0.134886</td>\\n\",\n",
      "-       \"      <td>0.106043</td>\\n\",\n",
      "-       \"      <td>0.128798</td>\\n\",\n",
      "-       \"      <td>4053</td>\\n\",\n",
      "-       \"      <td>1.689533</td>\\n\",\n",
      "-       \"      <td>39.199349</td>\\n\",\n",
      "-       \"      <td>32.018860</td>\\n\",\n",
      "-       \"      <td>-3.799293</td>\\n\",\n",
      "-       \"      <td>38.569600</td>\\n\",\n",
      "-       \"      <td>-1.501124</td>\\n\",\n",
      "-       \"      <td>26.167940</td>\\n\",\n",
      "-       \"      <td>20.572346</td>\\n\",\n",
      "-       \"      <td>24.986805</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>2</th>\\n\",\n",
      "-       \"      <td>4196</td>\\n\",\n",
      "-       \"      <td>0.204576</td>\\n\",\n",
      "-       \"      <td>0.192939</td>\\n\",\n",
      "-       \"      <td>0.156745</td>\\n\",\n",
      "-       \"      <td>-0.011448</td>\\n\",\n",
      "-       \"      <td>0.186642</td>\\n\",\n",
      "-       \"      <td>0.226347</td>\\n\",\n",
      "-       \"      <td>0.128658</td>\\n\",\n",
      "-       \"      <td>0.095656</td>\\n\",\n",
      "-       \"      <td>0.117090</td>\\n\",\n",
      "-       \"      <td>4196</td>\\n\",\n",
      "-       \"      <td>19.639275</td>\\n\",\n",
      "-       \"      <td>18.522111</td>\\n\",\n",
      "-       \"      <td>15.047543</td>\\n\",\n",
      "-       \"      <td>-1.099016</td>\\n\",\n",
      "-       \"      <td>17.917662</td>\\n\",\n",
      "-       \"      <td>21.729303</td>\\n\",\n",
      "-       \"      <td>12.351145</td>\\n\",\n",
      "-       \"      <td>9.183014</td>\\n\",\n",
      "-       \"      <td>11.240659</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>3</th>\\n\",\n",
      "-       \"      <td>4422</td>\\n\",\n",
      "-       \"      <td>-0.001838</td>\\n\",\n",
      "-       \"      <td>0.198717</td>\\n\",\n",
      "-       \"      <td>0.181343</td>\\n\",\n",
      "-       \"      <td>-0.002894</td>\\n\",\n",
      "-       \"      <td>0.215428</td>\\n\",\n",
      "-       \"      <td>-0.036878</td>\\n\",\n",
      "-       \"      <td>0.132829</td>\\n\",\n",
      "-       \"      <td>0.113879</td>\\n\",\n",
      "-       \"      <td>0.136089</td>\\n\",\n",
      "-       \"      <td>4422</td>\\n\",\n",
      "-       \"      <td>-0.104746</td>\\n\",\n",
      "-       \"      <td>11.326846</td>\\n\",\n",
      "-       \"      <td>10.336535</td>\\n\",\n",
      "-       \"      <td>-0.164956</td>\\n\",\n",
      "-       \"      <td>12.279409</td>\\n\",\n",
      "-       \"      <td>-2.102046</td>\\n\",\n",
      "-       \"      <td>7.571269</td>\\n\",\n",
      "-       \"      <td>6.491099</td>\\n\",\n",
      "-       \"      <td>7.757051</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>4</th>\\n\",\n",
      "-       \"      <td>4424</td>\\n\",\n",
      "-       \"      <td>-0.046762</td>\\n\",\n",
      "-       \"      <td>0.205482</td>\\n\",\n",
      "-       \"      <td>0.185708</td>\\n\",\n",
      "-       \"      <td>-0.005377</td>\\n\",\n",
      "-       \"      <td>0.218237</td>\\n\",\n",
      "-       \"      <td>-0.094255</td>\\n\",\n",
      "-       \"      <td>0.145178</td>\\n\",\n",
      "-       \"      <td>0.124556</td>\\n\",\n",
      "-       \"      <td>0.148208</td>\\n\",\n",
      "-       \"      <td>4424</td>\\n\",\n",
      "-       \"      <td>-1.730197</td>\\n\",\n",
      "-       \"      <td>7.602816</td>\\n\",\n",
      "-       \"      <td>6.871180</td>\\n\",\n",
      "-       \"      <td>-0.198932</td>\\n\",\n",
      "-       \"      <td>8.074769</td>\\n\",\n",
      "-       \"      <td>-3.487440</td>\\n\",\n",
      "-       \"      <td>5.371600</td>\\n\",\n",
      "-       \"      <td>4.608560</td>\\n\",\n",
      "-       \"      <td>5.483689</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>5</th>\\n\",\n",
      "-       \"      <td>3882</td>\\n\",\n",
      "-       \"      <td>0.000592</td>\\n\",\n",
      "-       \"      <td>0.224865</td>\\n\",\n",
      "-       \"      <td>0.189626</td>\\n\",\n",
      "-       \"      <td>0.062246</td>\\n\",\n",
      "-       \"      <td>0.233515</td>\\n\",\n",
      "-       \"      <td>-0.018172</td>\\n\",\n",
      "-       \"      <td>0.159256</td>\\n\",\n",
      "-       \"      <td>0.129561</td>\\n\",\n",
      "-       \"      <td>0.159715</td>\\n\",\n",
      "-       \"      <td>3882</td>\\n\",\n",
      "-       \"      <td>0.033144</td>\\n\",\n",
      "-       \"      <td>12.592452</td>\\n\",\n",
      "-       \"      <td>10.619075</td>\\n\",\n",
      "-       \"      <td>3.485752</td>\\n\",\n",
      "-       \"      <td>13.076824</td>\\n\",\n",
      "-       \"      <td>-1.017618</td>\\n\",\n",
      "-       \"      <td>8.918353</td>\\n\",\n",
      "-       \"      <td>7.255410</td>\\n\",\n",
      "-       \"      <td>8.944062</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>6</th>\\n\",\n",
      "-       \"      <td>4732</td>\\n\",\n",
      "-       \"      <td>0.256245</td>\\n\",\n",
      "-       \"      <td>0.177747</td>\\n\",\n",
      "-       \"      <td>0.141133</td>\\n\",\n",
      "-       \"      <td>-0.054281</td>\\n\",\n",
      "-       \"      <td>0.170076</td>\\n\",\n",
      "-       \"      <td>0.252110</td>\\n\",\n",
      "-       \"      <td>0.114163</td>\\n\",\n",
      "-       \"      <td>0.080882</td>\\n\",\n",
      "-       \"      <td>0.101907</td>\\n\",\n",
      "-       \"      <td>4732</td>\\n\",\n",
      "-       \"      <td>9.224836</td>\\n\",\n",
      "-       \"      <td>6.398888</td>\\n\",\n",
      "-       \"      <td>5.080800</td>\\n\",\n",
      "-       \"      <td>-1.954113</td>\\n\",\n",
      "-       \"      <td>6.122730</td>\\n\",\n",
      "-       \"      <td>9.075973</td>\\n\",\n",
      "-       \"      <td>4.109874</td>\\n\",\n",
      "-       \"      <td>2.911769</td>\\n\",\n",
      "-       \"      <td>3.668665</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>7</th>\\n\",\n",
      "-       \"      <td>3553</td>\\n\",\n",
      "-       \"      <td>0.036759</td>\\n\",\n",
      "-       \"      <td>0.119855</td>\\n\",\n",
      "-       \"      <td>0.116412</td>\\n\",\n",
      "-       \"      <td>-0.002644</td>\\n\",\n",
      "-       \"      <td>0.106559</td>\\n\",\n",
      "-       \"      <td>0.028544</td>\\n\",\n",
      "-       \"      <td>0.076208</td>\\n\",\n",
      "-       \"      <td>0.070953</td>\\n\",\n",
      "-       \"      <td>0.059992</td>\\n\",\n",
      "-       \"      <td>3553</td>\\n\",\n",
      "-       \"      <td>2.756912</td>\\n\",\n",
      "-       \"      <td>8.989140</td>\\n\",\n",
      "-       \"      <td>8.730864</td>\\n\",\n",
      "-       \"      <td>-0.198277</td>\\n\",\n",
      "-       \"      <td>7.991947</td>\\n\",\n",
      "-       \"      <td>2.140790</td>\\n\",\n",
      "-       \"      <td>5.715596</td>\\n\",\n",
      "-       \"      <td>5.321453</td>\\n\",\n",
      "-       \"      <td>4.499377</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>8</th>\\n\",\n",
      "-       \"      <td>3582</td>\\n\",\n",
      "-       \"      <td>0.115627</td>\\n\",\n",
      "-       \"      <td>0.216081</td>\\n\",\n",
      "-       \"      <td>0.176353</td>\\n\",\n",
      "-       \"      <td>-0.002795</td>\\n\",\n",
      "-       \"      <td>0.209227</td>\\n\",\n",
      "-       \"      <td>0.091589</td>\\n\",\n",
      "-       \"      <td>0.144046</td>\\n\",\n",
      "-       \"      <td>0.110570</td>\\n\",\n",
      "-       \"      <td>0.131569</td>\\n\",\n",
      "-       \"      <td>3582</td>\\n\",\n",
      "-       \"      <td>6.590739</td>\\n\",\n",
      "-       \"      <td>12.316622</td>\\n\",\n",
      "-       \"      <td>10.052107</td>\\n\",\n",
      "-       \"      <td>-0.159307</td>\\n\",\n",
      "-       \"      <td>11.925953</td>\\n\",\n",
      "-       \"      <td>5.220590</td>\\n\",\n",
      "-       \"      <td>8.210635</td>\\n\",\n",
      "-       \"      <td>6.302486</td>\\n\",\n",
      "-       \"      <td>7.499418</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>9</th>\\n\",\n",
      "-       \"      <td>3914</td>\\n\",\n",
      "-       \"      <td>0.172558</td>\\n\",\n",
      "-       \"      <td>0.315941</td>\\n\",\n",
      "-       \"      <td>0.291676</td>\\n\",\n",
      "-       \"      <td>-0.028690</td>\\n\",\n",
      "-       \"      <td>0.364501</td>\\n\",\n",
      "-       \"      <td>0.127867</td>\\n\",\n",
      "-       \"      <td>0.210205</td>\\n\",\n",
      "-       \"      <td>0.200281</td>\\n\",\n",
      "-       \"      <td>0.243347</td>\\n\",\n",
      "-       \"      <td>3914</td>\\n\",\n",
      "-       \"      <td>22.260043</td>\\n\",\n",
      "-       \"      <td>40.756373</td>\\n\",\n",
      "-       \"      <td>37.626256</td>\\n\",\n",
      "-       \"      <td>-3.701053</td>\\n\",\n",
      "-       \"      <td>47.020692</td>\\n\",\n",
      "-       \"      <td>16.494828</td>\\n\",\n",
      "-       \"      <td>27.116386</td>\\n\",\n",
      "-       \"      <td>25.836284</td>\\n\",\n",
      "-       \"      <td>31.391749</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>10</th>\\n\",\n",
      "-       \"      <td>3998</td>\\n\",\n",
      "-       \"      <td>-0.048620</td>\\n\",\n",
      "-       \"      <td>0.227125</td>\\n\",\n",
      "-       \"      <td>0.203715</td>\\n\",\n",
      "-       \"      <td>0.011323</td>\\n\",\n",
      "-       \"      <td>0.253654</td>\\n\",\n",
      "-       \"      <td>-0.082753</td>\\n\",\n",
      "-       \"      <td>0.151483</td>\\n\",\n",
      "-       \"      <td>0.132744</td>\\n\",\n",
      "-       \"      <td>0.163391</td>\\n\",\n",
      "-       \"      <td>3998</td>\\n\",\n",
      "-       \"      <td>-2.819972</td>\\n\",\n",
      "-       \"      <td>13.173279</td>\\n\",\n",
      "-       \"      <td>11.815479</td>\\n\",\n",
      "-       \"      <td>0.656753</td>\\n\",\n",
      "-       \"      <td>14.711919</td>\\n\",\n",
      "-       \"      <td>-4.799657</td>\\n\",\n",
      "-       \"      <td>8.786003</td>\\n\",\n",
      "-       \"      <td>7.699175</td>\\n\",\n",
      "-       \"      <td>9.476689</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>11</th>\\n\",\n",
      "-       \"      <td>4156</td>\\n\",\n",
      "-       \"      <td>-0.022582</td>\\n\",\n",
      "-       \"      <td>0.206010</td>\\n\",\n",
      "-       \"      <td>0.177684</td>\\n\",\n",
      "-       \"      <td>0.016386</td>\\n\",\n",
      "-       \"      <td>0.215706</td>\\n\",\n",
      "-       \"      <td>-0.059363</td>\\n\",\n",
      "-       \"      <td>0.146067</td>\\n\",\n",
      "-       \"      <td>0.122219</td>\\n\",\n",
      "-       \"      <td>0.147038</td>\\n\",\n",
      "-       \"      <td>4156</td>\\n\",\n",
      "-       \"      <td>-2.506617</td>\\n\",\n",
      "-       \"      <td>22.867165</td>\\n\",\n",
      "-       \"      <td>19.722978</td>\\n\",\n",
      "-       \"      <td>1.818880</td>\\n\",\n",
      "-       \"      <td>23.943365</td>\\n\",\n",
      "-       \"      <td>-6.589315</td>\\n\",\n",
      "-       \"      <td>16.213486</td>\\n\",\n",
      "-       \"      <td>13.566291</td>\\n\",\n",
      "-       \"      <td>16.321197</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>12</th>\\n\",\n",
      "-       \"      <td>4245</td>\\n\",\n",
      "-       \"      <td>0.002301</td>\\n\",\n",
      "-       \"      <td>0.192826</td>\\n\",\n",
      "-       \"      <td>0.184968</td>\\n\",\n",
      "-       \"      <td>0.015528</td>\\n\",\n",
      "-       \"      <td>0.213151</td>\\n\",\n",
      "-       \"      <td>-0.039060</td>\\n\",\n",
      "-       \"      <td>0.139228</td>\\n\",\n",
      "-       \"      <td>0.134442</td>\\n\",\n",
      "-       \"      <td>0.153583</td>\\n\",\n",
      "-       \"      <td>4245</td>\\n\",\n",
      "-       \"      <td>0.098945</td>\\n\",\n",
      "-       \"      <td>8.291498</td>\\n\",\n",
      "-       \"      <td>7.953635</td>\\n\",\n",
      "-       \"      <td>0.667709</td>\\n\",\n",
      "-       \"      <td>9.165502</td>\\n\",\n",
      "-       \"      <td>-1.679594</td>\\n\",\n",
      "-       \"      <td>5.986798</td>\\n\",\n",
      "-       \"      <td>5.781026</td>\\n\",\n",
      "-       \"      <td>6.604051</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>13</th>\\n\",\n",
      "-       \"      <td>4261</td>\\n\",\n",
      "-       \"      <td>0.000173</td>\\n\",\n",
      "-       \"      <td>0.181464</td>\\n\",\n",
      "-       \"      <td>0.164489</td>\\n\",\n",
      "-       \"      <td>0.012327</td>\\n\",\n",
      "-       \"      <td>0.182843</td>\\n\",\n",
      "-       \"      <td>-0.014316</td>\\n\",\n",
      "-       \"      <td>0.117521</td>\\n\",\n",
      "-       \"      <td>0.106494</td>\\n\",\n",
      "-       \"      <td>0.115234</td>\\n\",\n",
      "-       \"      <td>4261</td>\\n\",\n",
      "-       \"      <td>0.035886</td>\\n\",\n",
      "-       \"      <td>37.744584</td>\\n\",\n",
      "-       \"      <td>34.213611</td>\\n\",\n",
      "-       \"      <td>2.564059</td>\\n\",\n",
      "-       \"      <td>38.031383</td>\\n\",\n",
      "-       \"      <td>-2.977750</td>\\n\",\n",
      "-       \"      <td>24.444413</td>\\n\",\n",
      "-       \"      <td>22.150819</td>\\n\",\n",
      "-       \"      <td>23.968745</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>14</th>\\n\",\n",
      "-       \"      <td>4263</td>\\n\",\n",
      "-       \"      <td>0.132325</td>\\n\",\n",
      "-       \"      <td>0.221848</td>\\n\",\n",
      "-       \"      <td>0.214917</td>\\n\",\n",
      "-       \"      <td>0.032638</td>\\n\",\n",
      "-       \"      <td>0.261812</td>\\n\",\n",
      "-       \"      <td>0.148761</td>\\n\",\n",
      "-       \"      <td>0.139778</td>\\n\",\n",
      "-       \"      <td>0.143529</td>\\n\",\n",
      "-       \"      <td>0.170488</td>\\n\",\n",
      "-       \"      <td>4263</td>\\n\",\n",
      "-       \"      <td>13.232466</td>\\n\",\n",
      "-       \"      <td>22.184790</td>\\n\",\n",
      "-       \"      <td>21.491711</td>\\n\",\n",
      "-       \"      <td>3.263830</td>\\n\",\n",
      "-       \"      <td>26.181188</td>\\n\",\n",
      "-       \"      <td>14.876100</td>\\n\",\n",
      "-       \"      <td>13.977789</td>\\n\",\n",
      "-       \"      <td>14.352853</td>\\n\",\n",
      "-       \"      <td>17.048847</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>15</th>\\n\",\n",
      "-       \"      <td>5246</td>\\n\",\n",
      "-       \"      <td>0.005992</td>\\n\",\n",
      "-       \"      <td>0.214139</td>\\n\",\n",
      "-       \"      <td>0.202626</td>\\n\",\n",
      "-       \"      <td>-0.012471</td>\\n\",\n",
      "-       \"      <td>0.232215</td>\\n\",\n",
      "-       \"      <td>-0.005717</td>\\n\",\n",
      "-       \"      <td>0.148833</td>\\n\",\n",
      "-       \"      <td>0.137918</td>\\n\",\n",
      "-       \"      <td>0.155673</td>\\n\",\n",
      "-       \"      <td>5246</td>\\n\",\n",
      "-       \"      <td>0.497374</td>\\n\",\n",
      "-       \"      <td>17.773546</td>\\n\",\n",
      "-       \"      <td>16.817932</td>\\n\",\n",
      "-       \"      <td>-1.035112</td>\\n\",\n",
      "-       \"      <td>19.273876</td>\\n\",\n",
      "-       \"      <td>-0.474534</td>\\n\",\n",
      "-       \"      <td>12.353119</td>\\n\",\n",
      "-       \"      <td>11.447228</td>\\n\",\n",
      "-       \"      <td>12.920865</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>16</th>\\n\",\n",
      "-       \"      <td>3502</td>\\n\",\n",
      "-       \"      <td>0.185189</td>\\n\",\n",
      "-       \"      <td>0.218130</td>\\n\",\n",
      "-       \"      <td>0.187834</td>\\n\",\n",
      "-       \"      <td>-0.031854</td>\\n\",\n",
      "-       \"      <td>0.229086</td>\\n\",\n",
      "-       \"      <td>0.224326</td>\\n\",\n",
      "-       \"      <td>0.143763</td>\\n\",\n",
      "-       \"      <td>0.120377</td>\\n\",\n",
      "-       \"      <td>0.147329</td>\\n\",\n",
      "-       \"      <td>3502</td>\\n\",\n",
      "-       \"      <td>46.297236</td>\\n\",\n",
      "-       \"      <td>54.532542</td>\\n\",\n",
      "-       \"      <td>46.958449</td>\\n\",\n",
      "-       \"      <td>-7.963532</td>\\n\",\n",
      "-       \"      <td>57.271522</td>\\n\",\n",
      "-       \"      <td>56.081377</td>\\n\",\n",
      "-       \"      <td>35.940792</td>\\n\",\n",
      "-       \"      <td>30.094137</td>\\n\",\n",
      "-       \"      <td>36.832132</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>17</th>\\n\",\n",
      "-       \"      <td>4285</td>\\n\",\n",
      "-       \"      <td>0.150757</td>\\n\",\n",
      "-       \"      <td>0.256606</td>\\n\",\n",
      "-       \"      <td>0.213728</td>\\n\",\n",
      "-       \"      <td>-0.008046</td>\\n\",\n",
      "-       \"      <td>0.262016</td>\\n\",\n",
      "-       \"      <td>0.169033</td>\\n\",\n",
      "-       \"      <td>0.176817</td>\\n\",\n",
      "-       \"      <td>0.136219</td>\\n\",\n",
      "-       \"      <td>0.169356</td>\\n\",\n",
      "-       \"      <td>4285</td>\\n\",\n",
      "-       \"      <td>18.995438</td>\\n\",\n",
      "-       \"      <td>32.332398</td>\\n\",\n",
      "-       \"      <td>26.929670</td>\\n\",\n",
      "-       \"      <td>-1.013785</td>\\n\",\n",
      "-       \"      <td>33.014027</td>\\n\",\n",
      "-       \"      <td>21.298105</td>\\n\",\n",
      "-       \"      <td>22.278980</td>\\n\",\n",
      "-       \"      <td>17.163532</td>\\n\",\n",
      "-       \"      <td>21.338797</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>18</th>\\n\",\n",
      "-       \"      <td>4777</td>\\n\",\n",
      "-       \"      <td>0.026046</td>\\n\",\n",
      "-       \"      <td>0.225686</td>\\n\",\n",
      "-       \"      <td>0.191275</td>\\n\",\n",
      "-       \"      <td>0.043158</td>\\n\",\n",
      "-       \"      <td>0.244065</td>\\n\",\n",
      "-       \"      <td>-0.004380</td>\\n\",\n",
      "-       \"      <td>0.151358</td>\\n\",\n",
      "-       \"      <td>0.126596</td>\\n\",\n",
      "-       \"      <td>0.161171</td>\\n\",\n",
      "-       \"      <td>4777</td>\\n\",\n",
      "-       \"      <td>2.682757</td>\\n\",\n",
      "-       \"      <td>23.245624</td>\\n\",\n",
      "-       \"      <td>19.701309</td>\\n\",\n",
      "-       \"      <td>4.445239</td>\\n\",\n",
      "-       \"      <td>25.138662</td>\\n\",\n",
      "-       \"      <td>-0.451171</td>\\n\",\n",
      "-       \"      <td>15.589867</td>\\n\",\n",
      "-       \"      <td>13.039338</td>\\n\",\n",
      "-       \"      <td>16.600586</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>19</th>\\n\",\n",
      "-       \"      <td>3804</td>\\n\",\n",
      "-       \"      <td>0.009016</td>\\n\",\n",
      "-       \"      <td>0.224911</td>\\n\",\n",
      "-       \"      <td>0.210744</td>\\n\",\n",
      "-       \"      <td>0.018820</td>\\n\",\n",
      "-       \"      <td>0.233472</td>\\n\",\n",
      "-       \"      <td>-0.012432</td>\\n\",\n",
      "-       \"      <td>0.152079</td>\\n\",\n",
      "-       \"      <td>0.138293</td>\\n\",\n",
      "-       \"      <td>0.152445</td>\\n\",\n",
      "-       \"      <td>3804</td>\\n\",\n",
      "-       \"      <td>2.172749</td>\\n\",\n",
      "-       \"      <td>54.203640</td>\\n\",\n",
      "-       \"      <td>50.789287</td>\\n\",\n",
      "-       \"      <td>4.535715</td>\\n\",\n",
      "-       \"      <td>56.266839</td>\\n\",\n",
      "-       \"      <td>-2.996087</td>\\n\",\n",
      "-       \"      <td>36.651009</td>\\n\",\n",
      "-       \"      <td>33.328667</td>\\n\",\n",
      "-       \"      <td>36.739267</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>20</th>\\n\",\n",
      "-       \"      <td>4000</td>\\n\",\n",
      "-       \"      <td>0.154955</td>\\n\",\n",
      "-       \"      <td>0.226454</td>\\n\",\n",
      "-       \"      <td>0.199042</td>\\n\",\n",
      "-       \"      <td>-0.002604</td>\\n\",\n",
      "-       \"      <td>0.236806</td>\\n\",\n",
      "-       \"      <td>0.122039</td>\\n\",\n",
      "-       \"      <td>0.150495</td>\\n\",\n",
      "-       \"      <td>0.128195</td>\\n\",\n",
      "-       \"      <td>0.153474</td>\\n\",\n",
      "-       \"      <td>4000</td>\\n\",\n",
      "-       \"      <td>40.133228</td>\\n\",\n",
      "-       \"      <td>58.651510</td>\\n\",\n",
      "-       \"      <td>51.551967</td>\\n\",\n",
      "-       \"      <td>-0.674322</td>\\n\",\n",
      "-       \"      <td>61.332815</td>\\n\",\n",
      "-       \"      <td>31.608018</td>\\n\",\n",
      "-       \"      <td>38.978228</td>\\n\",\n",
      "-       \"      <td>33.202487</td>\\n\",\n",
      "-       \"      <td>39.749831</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>21</th>\\n\",\n",
      "-       \"      <td>4189</td>\\n\",\n",
      "-       \"      <td>-0.015730</td>\\n\",\n",
      "-       \"      <td>0.219537</td>\\n\",\n",
      "-       \"      <td>0.190627</td>\\n\",\n",
      "-       \"      <td>0.012397</td>\\n\",\n",
      "-       \"      <td>0.233876</td>\\n\",\n",
      "-       \"      <td>-0.044362</td>\\n\",\n",
      "-       \"      <td>0.154094</td>\\n\",\n",
      "-       \"      <td>0.122526</td>\\n\",\n",
      "-       \"      <td>0.154364</td>\\n\",\n",
      "-       \"      <td>4189</td>\\n\",\n",
      "-       \"      <td>-2.029133</td>\\n\",\n",
      "-       \"      <td>28.320295</td>\\n\",\n",
      "-       \"      <td>24.590874</td>\\n\",\n",
      "-       \"      <td>1.599174</td>\\n\",\n",
      "-       \"      <td>30.170006</td>\\n\",\n",
      "-       \"      <td>-5.722714</td>\\n\",\n",
      "-       \"      <td>19.878113</td>\\n\",\n",
      "-       \"      <td>15.805897</td>\\n\",\n",
      "-       \"      <td>19.912967</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>22</th>\\n\",\n",
      "-       \"      <td>4262</td>\\n\",\n",
      "-       \"      <td>0.188380</td>\\n\",\n",
      "-       \"      <td>0.249994</td>\\n\",\n",
      "-       \"      <td>0.224966</td>\\n\",\n",
      "-       \"      <td>0.034883</td>\\n\",\n",
      "-       \"      <td>0.278707</td>\\n\",\n",
      "-       \"      <td>0.225496</td>\\n\",\n",
      "-       \"      <td>0.168260</td>\\n\",\n",
      "-       \"      <td>0.155327</td>\\n\",\n",
      "-       \"      <td>0.190923</td>\\n\",\n",
      "-       \"      <td>4262</td>\\n\",\n",
      "-       \"      <td>36.734185</td>\\n\",\n",
      "-       \"      <td>48.748778</td>\\n\",\n",
      "-       \"      <td>43.868298</td>\\n\",\n",
      "-       \"      <td>6.802155</td>\\n\",\n",
      "-       \"      <td>54.347836</td>\\n\",\n",
      "-       \"      <td>43.971693</td>\\n\",\n",
      "-       \"      <td>32.810705</td>\\n\",\n",
      "-       \"      <td>30.288685</td>\\n\",\n",
      "-       \"      <td>37.229972</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>23</th>\\n\",\n",
      "-       \"      <td>4284</td>\\n\",\n",
      "-       \"      <td>0.028524</td>\\n\",\n",
      "-       \"      <td>0.229816</td>\\n\",\n",
      "-       \"      <td>0.198722</td>\\n\",\n",
      "-       \"      <td>0.011167</td>\\n\",\n",
      "-       \"      <td>0.236951</td>\\n\",\n",
      "-       \"      <td>0.004692</td>\\n\",\n",
      "-       \"      <td>0.166090</td>\\n\",\n",
      "-       \"      <td>0.136053</td>\\n\",\n",
      "-       \"      <td>0.162269</td>\\n\",\n",
      "-       \"      <td>4284</td>\\n\",\n",
      "-       \"      <td>0.570486</td>\\n\",\n",
      "-       \"      <td>4.596328</td>\\n\",\n",
      "-       \"      <td>3.974448</td>\\n\",\n",
      "-       \"      <td>0.223334</td>\\n\",\n",
      "-       \"      <td>4.739017</td>\\n\",\n",
      "-       \"      <td>0.093841</td>\\n\",\n",
      "-       \"      <td>3.321808</td>\\n\",\n",
      "-       \"      <td>2.721057</td>\\n\",\n",
      "-       \"      <td>3.245384</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>24</th>\\n\",\n",
      "-       \"      <td>4426</td>\\n\",\n",
      "-       \"      <td>-0.009749</td>\\n\",\n",
      "-       \"      <td>0.224826</td>\\n\",\n",
      "-       \"      <td>0.213394</td>\\n\",\n",
      "-       \"      <td>0.000653</td>\\n\",\n",
      "-       \"      <td>0.251528</td>\\n\",\n",
      "-       \"      <td>-0.049095</td>\\n\",\n",
      "-       \"      <td>0.150299</td>\\n\",\n",
      "-       \"      <td>0.140670</td>\\n\",\n",
      "-       \"      <td>0.161438</td>\\n\",\n",
      "-       \"      <td>4426</td>\\n\",\n",
      "-       \"      <td>-2.144793</td>\\n\",\n",
      "-       \"      <td>49.461620</td>\\n\",\n",
      "-       \"      <td>46.946593</td>\\n\",\n",
      "-       \"      <td>0.143648</td>\\n\",\n",
      "-       \"      <td>55.336220</td>\\n\",\n",
      "-       \"      <td>-10.800926</td>\\n\",\n",
      "-       \"      <td>33.065763</td>\\n\",\n",
      "-       \"      <td>30.947375</td>\\n\",\n",
      "-       \"      <td>35.516400</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"  </tbody>\\n\",\n",
      "-       \"</table>\\n\",\n",
      "-       \"</div>\"\n",
      "-      ],\n",
      "-      \"text/plain\": [\n",
      "-       \"     IDX  mean_ndvi_2022-06-02  mean_ndvi_2022-06-17  mean_ndvi_2022-06-07  \\\\\\n\",\n",
      "-       \"0   3942              0.007476              0.212770              0.180844   \\n\",\n",
      "-       \"1   4053              0.008709              0.202058              0.165046   \\n\",\n",
      "-       \"2   4196              0.204576              0.192939              0.156745   \\n\",\n",
      "-       \"3   4422             -0.001838              0.198717              0.181343   \\n\",\n",
      "-       \"4   4424             -0.046762              0.205482              0.185708   \\n\",\n",
      "-       \"5   3882              0.000592              0.224865              0.189626   \\n\",\n",
      "-       \"6   4732              0.256245              0.177747              0.141133   \\n\",\n",
      "-       \"7   3553              0.036759              0.119855              0.116412   \\n\",\n",
      "-       \"8   3582              0.115627              0.216081              0.176353   \\n\",\n",
      "-       \"9   3914              0.172558              0.315941              0.291676   \\n\",\n",
      "-       \"10  3998             -0.048620              0.227125              0.203715   \\n\",\n",
      "-       \"11  4156             -0.022582              0.206010              0.177684   \\n\",\n",
      "-       \"12  4245              0.002301              0.192826              0.184968   \\n\",\n",
      "-       \"13  4261              0.000173              0.181464              0.164489   \\n\",\n",
      "-       \"14  4263              0.132325              0.221848              0.214917   \\n\",\n",
      "-       \"15  5246              0.005992              0.214139              0.202626   \\n\",\n",
      "-       \"16  3502              0.185189              0.218130              0.187834   \\n\",\n",
      "-       \"17  4285              0.150757              0.256606              0.213728   \\n\",\n",
      "-       \"18  4777              0.026046              0.225686              0.191275   \\n\",\n",
      "-       \"19  3804              0.009016              0.224911              0.210744   \\n\",\n",
      "-       \"20  4000              0.154955              0.226454              0.199042   \\n\",\n",
      "-       \"21  4189             -0.015730              0.219537              0.190627   \\n\",\n",
      "-       \"22  4262              0.188380              0.249994              0.224966   \\n\",\n",
      "-       \"23  4284              0.028524              0.229816              0.198722   \\n\",\n",
      "-       \"24  4426             -0.009749              0.224826              0.213394   \\n\",\n",
      "-       \"\\n\",\n",
      "-       \"    mean_ndvi_2022-06-22  mean_ndvi_2022-06-12  mean_ndre_2022-06-02  \\\\\\n\",\n",
      "-       \"0               0.038478              0.219896             -0.017073   \\n\",\n",
      "-       \"1              -0.019584              0.198812             -0.007738   \\n\",\n",
      "-       \"2              -0.011448              0.186642              0.226347   \\n\",\n",
      "-       \"3              -0.002894              0.215428             -0.036878   \\n\",\n",
      "-       \"4              -0.005377              0.218237             -0.094255   \\n\",\n",
      "-       \"5               0.062246              0.233515             -0.018172   \\n\",\n",
      "-       \"6              -0.054281              0.170076              0.252110   \\n\",\n",
      "-       \"7              -0.002644              0.106559              0.028544   \\n\",\n",
      "-       \"8              -0.002795              0.209227              0.091589   \\n\",\n",
      "-       \"9              -0.028690              0.364501              0.127867   \\n\",\n",
      "-       \"10              0.011323              0.253654             -0.082753   \\n\",\n",
      "-       \"11              0.016386              0.215706             -0.059363   \\n\",\n",
      "-       \"12              0.015528              0.213151             -0.039060   \\n\",\n",
      "-       \"13              0.012327              0.182843             -0.014316   \\n\",\n",
      "-       \"14              0.032638              0.261812              0.148761   \\n\",\n",
      "-       \"15             -0.012471              0.232215             -0.005717   \\n\",\n",
      "-       \"16             -0.031854              0.229086              0.224326   \\n\",\n",
      "-       \"17             -0.008046              0.262016              0.169033   \\n\",\n",
      "-       \"18              0.043158              0.244065             -0.004380   \\n\",\n",
      "-       \"19              0.018820              0.233472             -0.012432   \\n\",\n",
      "-       \"20             -0.002604              0.236806              0.122039   \\n\",\n",
      "-       \"21              0.012397              0.233876             -0.044362   \\n\",\n",
      "-       \"22              0.034883              0.278707              0.225496   \\n\",\n",
      "-       \"23              0.011167              0.236951              0.004692   \\n\",\n",
      "-       \"24              0.000653              0.251528             -0.049095   \\n\",\n",
      "-       \"\\n\",\n",
      "-       \"    mean_ndre_2022-06-17  mean_ndre_2022-06-07  mean_ndre_2022-06-12  IDX.1  \\\\\\n\",\n",
      "-       \"0               0.151714              0.123746              0.153333   3942   \\n\",\n",
      "-       \"1               0.134886              0.106043              0.128798   4053   \\n\",\n",
      "-       \"2               0.128658              0.095656              0.117090   4196   \\n\",\n",
      "-       \"3               0.132829              0.113879              0.136089   4422   \\n\",\n",
      "-       \"4               0.145178              0.124556              0.148208   4424   \\n\",\n",
      "-       \"5               0.159256              0.129561              0.159715   3882   \\n\",\n",
      "-       \"6               0.114163              0.080882              0.101907   4732   \\n\",\n",
      "-       \"7               0.076208              0.070953              0.059992   3553   \\n\",\n",
      "-       \"8               0.144046              0.110570              0.131569   3582   \\n\",\n",
      "-       \"9               0.210205              0.200281              0.243347   3914   \\n\",\n",
      "-       \"10              0.151483              0.132744              0.163391   3998   \\n\",\n",
      "-       \"11              0.146067              0.122219              0.147038   4156   \\n\",\n",
      "-       \"12              0.139228              0.134442              0.153583   4245   \\n\",\n",
      "-       \"13              0.117521              0.106494              0.115234   4261   \\n\",\n",
      "-       \"14              0.139778              0.143529              0.170488   4263   \\n\",\n",
      "-       \"15              0.148833              0.137918              0.155673   5246   \\n\",\n",
      "-       \"16              0.143763              0.120377              0.147329   3502   \\n\",\n",
      "-       \"17              0.176817              0.136219              0.169356   4285   \\n\",\n",
      "-       \"18              0.151358              0.126596              0.161171   4777   \\n\",\n",
      "-       \"19              0.152079              0.138293              0.152445   3804   \\n\",\n",
      "-       \"20              0.150495              0.128195              0.153474   4000   \\n\",\n",
      "-       \"21              0.154094              0.122526              0.154364   4189   \\n\",\n",
      "-       \"22              0.168260              0.155327              0.190923   4262   \\n\",\n",
      "-       \"23              0.166090              0.136053              0.162269   4284   \\n\",\n",
      "-       \"24              0.150299              0.140670              0.161438   4426   \\n\",\n",
      "-       \"\\n\",\n",
      "-       \"    sum_ndvi_2022-06-02  sum_ndvi_2022-06-17  sum_ndvi_2022-06-07  \\\\\\n\",\n",
      "-       \"0              0.530825            15.106694            12.839933   \\n\",\n",
      "-       \"1              1.689533            39.199349            32.018860   \\n\",\n",
      "-       \"2             19.639275            18.522111            15.047543   \\n\",\n",
      "-       \"3             -0.104746            11.326846            10.336535   \\n\",\n",
      "-       \"4             -1.730197             7.602816             6.871180   \\n\",\n",
      "-       \"5              0.033144            12.592452            10.619075   \\n\",\n",
      "-       \"6              9.224836             6.398888             5.080800   \\n\",\n",
      "-       \"7              2.756912             8.989140             8.730864   \\n\",\n",
      "-       \"8              6.590739            12.316622            10.052107   \\n\",\n",
      "-       \"9             22.260043            40.756373            37.626256   \\n\",\n",
      "-       \"10            -2.819972            13.173279            11.815479   \\n\",\n",
      "-       \"11            -2.506617            22.867165            19.722978   \\n\",\n",
      "-       \"12             0.098945             8.291498             7.953635   \\n\",\n",
      "-       \"13             0.035886            37.744584            34.213611   \\n\",\n",
      "-       \"14            13.232466            22.184790            21.491711   \\n\",\n",
      "-       \"15             0.497374            17.773546            16.817932   \\n\",\n",
      "-       \"16            46.297236            54.532542            46.958449   \\n\",\n",
      "-       \"17            18.995438            32.332398            26.929670   \\n\",\n",
      "-       \"18             2.682757            23.245624            19.701309   \\n\",\n",
      "-       \"19             2.172749            54.203640            50.789287   \\n\",\n",
      "-       \"20            40.133228            58.651510            51.551967   \\n\",\n",
      "-       \"21            -2.029133            28.320295            24.590874   \\n\",\n",
      "-       \"22            36.734185            48.748778            43.868298   \\n\",\n",
      "-       \"23             0.570486             4.596328             3.974448   \\n\",\n",
      "-       \"24            -2.144793            49.461620            46.946593   \\n\",\n",
      "-       \"\\n\",\n",
      "-       \"    sum_ndvi_2022-06-22  sum_ndvi_2022-06-12  sum_ndre_2022-06-02  \\\\\\n\",\n",
      "-       \"0              2.731913            15.612608            -1.212181   \\n\",\n",
      "-       \"1             -3.799293            38.569600            -1.501124   \\n\",\n",
      "-       \"2             -1.099016            17.917662            21.729303   \\n\",\n",
      "-       \"3             -0.164956            12.279409            -2.102046   \\n\",\n",
      "-       \"4             -0.198932             8.074769            -3.487440   \\n\",\n",
      "-       \"5              3.485752            13.076824            -1.017618   \\n\",\n",
      "-       \"6             -1.954113             6.122730             9.075973   \\n\",\n",
      "-       \"7             -0.198277             7.991947             2.140790   \\n\",\n",
      "-       \"8             -0.159307            11.925953             5.220590   \\n\",\n",
      "-       \"9             -3.701053            47.020692            16.494828   \\n\",\n",
      "-       \"10             0.656753            14.711919            -4.799657   \\n\",\n",
      "-       \"11             1.818880            23.943365            -6.589315   \\n\",\n",
      "-       \"12             0.667709             9.165502            -1.679594   \\n\",\n",
      "-       \"13             2.564059            38.031383            -2.977750   \\n\",\n",
      "-       \"14             3.263830            26.181188            14.876100   \\n\",\n",
      "-       \"15            -1.035112            19.273876            -0.474534   \\n\",\n",
      "-       \"16            -7.963532            57.271522            56.081377   \\n\",\n",
      "-       \"17            -1.013785            33.014027            21.298105   \\n\",\n",
      "-       \"18             4.445239            25.138662            -0.451171   \\n\",\n",
      "-       \"19             4.535715            56.266839            -2.996087   \\n\",\n",
      "-       \"20            -0.674322            61.332815            31.608018   \\n\",\n",
      "-       \"21             1.599174            30.170006            -5.722714   \\n\",\n",
      "-       \"22             6.802155            54.347836            43.971693   \\n\",\n",
      "-       \"23             0.223334             4.739017             0.093841   \\n\",\n",
      "-       \"24             0.143648            55.336220           -10.800926   \\n\",\n",
      "-       \"\\n\",\n",
      "-       \"    sum_ndre_2022-06-17  sum_ndre_2022-06-07  sum_ndre_2022-06-12  \\n\",\n",
      "-       \"0             10.771678             8.785966            10.886635  \\n\",\n",
      "-       \"1             26.167940            20.572346            24.986805  \\n\",\n",
      "-       \"2             12.351145             9.183014            11.240659  \\n\",\n",
      "-       \"3              7.571269             6.491099             7.757051  \\n\",\n",
      "-       \"4              5.371600             4.608560             5.483689  \\n\",\n",
      "-       \"5              8.918353             7.255410             8.944062  \\n\",\n",
      "-       \"6              4.109874             2.911769             3.668665  \\n\",\n",
      "-       \"7              5.715596             5.321453             4.499377  \\n\",\n",
      "-       \"8              8.210635             6.302486             7.499418  \\n\",\n",
      "-       \"9             27.116386            25.836284            31.391749  \\n\",\n",
      "-       \"10             8.786003             7.699175             9.476689  \\n\",\n",
      "-       \"11            16.213486            13.566291            16.321197  \\n\",\n",
      "-       \"12             5.986798             5.781026             6.604051  \\n\",\n",
      "-       \"13            24.444413            22.150819            23.968745  \\n\",\n",
      "-       \"14            13.977789            14.352853            17.048847  \\n\",\n",
      "-       \"15            12.353119            11.447228            12.920865  \\n\",\n",
      "-       \"16            35.940792            30.094137            36.832132  \\n\",\n",
      "-       \"17            22.278980            17.163532            21.338797  \\n\",\n",
      "-       \"18            15.589867            13.039338            16.600586  \\n\",\n",
      "-       \"19            36.651009            33.328667            36.739267  \\n\",\n",
      "-       \"20            38.978228            33.202487            39.749831  \\n\",\n",
      "-       \"21            19.878113            15.805897            19.912967  \\n\",\n",
      "-       \"22            32.810705            30.288685            37.229972  \\n\",\n",
      "-       \"23             3.321808             2.721057             3.245384  \\n\",\n",
      "-       \"24            33.065763            30.947375            35.516400  \"\n",
      "-      ]\n",
      "-     },\n",
      "-     \"execution_count\": 15,\n",
      "-     \"metadata\": {},\n",
      "-     \"output_type\": \"execute_result\"\n",
      "-    }\n",
      "-   ],\n",
      "-   \"source\": [\n",
      "-    \"df = pd.read_csv(r'./preprocess.csv')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"df\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 58,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"produccion = pd.read_excel(r'C:\\\\Users\\\\reuni\\\\Downloads\\\\producciones(1).xlsx')\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 60,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"dfdf = pd.merge(pp, df, left_on='idx', right_on='IDX')\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 63,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"dfdf_p = pd.merge(dfdf, produccion, left_on='Parcela',right_on='ID')\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 75,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [\n",
      "-    {\n",
      "-     \"data\": {\n",
      "-      \"text/html\": [\n",
      "-       \"<div>\\n\",\n",
      "-       \"<style scoped>\\n\",\n",
      "-       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
      "-       \"        vertical-align: middle;\\n\",\n",
      "-       \"    }\\n\",\n",
      "-       \"\\n\",\n",
      "-       \"    .dataframe tbody tr th {\\n\",\n",
      "-       \"        vertical-align: top;\\n\",\n",
      "-       \"    }\\n\",\n",
      "-       \"\\n\",\n",
      "-       \"    .dataframe thead th {\\n\",\n",
      "-       \"        text-align: right;\\n\",\n",
      "-       \"    }\\n\",\n",
      "-       \"</style>\\n\",\n",
      "-       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
      "-       \"  <thead>\\n\",\n",
      "-       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
      "-       \"      <th></th>\\n\",\n",
      "-       \"      <th>Parcela</th>\\n\",\n",
      "-       \"      <th>idx</th>\\n\",\n",
      "-       \"      <th>PROD</th>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"  </thead>\\n\",\n",
      "-       \"  <tbody>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>0</th>\\n\",\n",
      "-       \"      <td>FUEN34-16</td>\\n\",\n",
      "-       \"      <td>3502</td>\\n\",\n",
      "-       \"      <td>31595</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>1</th>\\n\",\n",
      "-       \"      <td>FUEN34-11</td>\\n\",\n",
      "-       \"      <td>3553</td>\\n\",\n",
      "-       \"      <td>3285</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>2</th>\\n\",\n",
      "-       \"      <td>FUEN34-21</td>\\n\",\n",
      "-       \"      <td>3582</td>\\n\",\n",
      "-       \"      <td>2581</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>3</th>\\n\",\n",
      "-       \"      <td>FUEN34-4</td>\\n\",\n",
      "-       \"      <td>3804</td>\\n\",\n",
      "-       \"      <td>13155</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>4</th>\\n\",\n",
      "-       \"      <td>NAVA05-22</td>\\n\",\n",
      "-       \"      <td>3882</td>\\n\",\n",
      "-       \"      <td>2487</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>5</th>\\n\",\n",
      "-       \"      <td>FUEN31-30</td>\\n\",\n",
      "-       \"      <td>3914</td>\\n\",\n",
      "-       \"      <td>0</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>6</th>\\n\",\n",
      "-       \"      <td>NAVA05-24</td>\\n\",\n",
      "-       \"      <td>3942</td>\\n\",\n",
      "-       \"      <td>3843</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>7</th>\\n\",\n",
      "-       \"      <td>FUEN34-15</td>\\n\",\n",
      "-       \"      <td>3998</td>\\n\",\n",
      "-       \"      <td>2537</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>8</th>\\n\",\n",
      "-       \"      <td>FUEN34-5</td>\\n\",\n",
      "-       \"      <td>4000</td>\\n\",\n",
      "-       \"      <td>14912</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>9</th>\\n\",\n",
      "-       \"      <td>NAVA05-3</td>\\n\",\n",
      "-       \"      <td>4053</td>\\n\",\n",
      "-       \"      <td>10374</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>10</th>\\n\",\n",
      "-       \"      <td>FUEN31-5</td>\\n\",\n",
      "-       \"      <td>4156</td>\\n\",\n",
      "-       \"      <td>5482</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>11</th>\\n\",\n",
      "-       \"      <td>FUEN31-6</td>\\n\",\n",
      "-       \"      <td>4189</td>\\n\",\n",
      "-       \"      <td>6076</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>12</th>\\n\",\n",
      "-       \"      <td>FUEN31-14</td>\\n\",\n",
      "-       \"      <td>4196</td>\\n\",\n",
      "-       \"      <td>4489</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>13</th>\\n\",\n",
      "-       \"      <td>FUEN34-24</td>\\n\",\n",
      "-       \"      <td>4245</td>\\n\",\n",
      "-       \"      <td>1235</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>14</th>\\n\",\n",
      "-       \"      <td>FUEN34-0</td>\\n\",\n",
      "-       \"      <td>4261</td>\\n\",\n",
      "-       \"      <td>12240</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>15</th>\\n\",\n",
      "-       \"      <td>FUEN34-1</td>\\n\",\n",
      "-       \"      <td>4262</td>\\n\",\n",
      "-       \"      <td>10599</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>16</th>\\n\",\n",
      "-       \"      <td>FUEN34-1.1</td>\\n\",\n",
      "-       \"      <td>4263</td>\\n\",\n",
      "-       \"      <td>4426</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>17</th>\\n\",\n",
      "-       \"      <td>FUEN34-25</td>\\n\",\n",
      "-       \"      <td>4284</td>\\n\",\n",
      "-       \"      <td>691</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>18</th>\\n\",\n",
      "-       \"      <td>FUEN31-36</td>\\n\",\n",
      "-       \"      <td>4285</td>\\n\",\n",
      "-       \"      <td>5631</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>19</th>\\n\",\n",
      "-       \"      <td>FUEN34-8</td>\\n\",\n",
      "-       \"      <td>4422</td>\\n\",\n",
      "-       \"      <td>2173</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>20</th>\\n\",\n",
      "-       \"      <td>FUEN39-8</td>\\n\",\n",
      "-       \"      <td>4424</td>\\n\",\n",
      "-       \"      <td>1729</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>21</th>\\n\",\n",
      "-       \"      <td>FUEN34-10</td>\\n\",\n",
      "-       \"      <td>4426</td>\\n\",\n",
      "-       \"      <td>12134</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>22</th>\\n\",\n",
      "-       \"      <td>FUEN34-23</td>\\n\",\n",
      "-       \"      <td>4732</td>\\n\",\n",
      "-       \"      <td>3248</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>23</th>\\n\",\n",
      "-       \"      <td>NAVA05-21</td>\\n\",\n",
      "-       \"      <td>4777</td>\\n\",\n",
      "-       \"      <td>4940</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>24</th>\\n\",\n",
      "-       \"      <td>NAVA05-86</td>\\n\",\n",
      "-       \"      <td>5246</td>\\n\",\n",
      "-       \"      <td>4322</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"  </tbody>\\n\",\n",
      "-       \"</table>\\n\",\n",
      "-       \"</div>\"\n",
      "-      ],\n",
      "-      \"text/plain\": [\n",
      "-       \"       Parcela   idx   PROD\\n\",\n",
      "-       \"0    FUEN34-16  3502  31595\\n\",\n",
      "-       \"1    FUEN34-11  3553   3285\\n\",\n",
      "-       \"2    FUEN34-21  3582   2581\\n\",\n",
      "-       \"3     FUEN34-4  3804  13155\\n\",\n",
      "-       \"4    NAVA05-22  3882   2487\\n\",\n",
      "-       \"5    FUEN31-30  3914      0\\n\",\n",
      "-       \"6    NAVA05-24  3942   3843\\n\",\n",
      "-       \"7    FUEN34-15  3998   2537\\n\",\n",
      "-       \"8     FUEN34-5  4000  14912\\n\",\n",
      "-       \"9     NAVA05-3  4053  10374\\n\",\n",
      "-       \"10    FUEN31-5  4156   5482\\n\",\n",
      "-       \"11    FUEN31-6  4189   6076\\n\",\n",
      "-       \"12   FUEN31-14  4196   4489\\n\",\n",
      "-       \"13   FUEN34-24  4245   1235\\n\",\n",
      "-       \"14    FUEN34-0  4261  12240\\n\",\n",
      "-       \"15    FUEN34-1  4262  10599\\n\",\n",
      "-       \"16  FUEN34-1.1  4263   4426\\n\",\n",
      "-       \"17   FUEN34-25  4284    691\\n\",\n",
      "-       \"18   FUEN31-36  4285   5631\\n\",\n",
      "-       \"19    FUEN34-8  4422   2173\\n\",\n",
      "-       \"20    FUEN39-8  4424   1729\\n\",\n",
      "-       \"21   FUEN34-10  4426  12134\\n\",\n",
      "-       \"22   FUEN34-23  4732   3248\\n\",\n",
      "-       \"23   NAVA05-21  4777   4940\\n\",\n",
      "-       \"24   NAVA05-86  5246   4322\"\n",
      "-      ]\n",
      "-     },\n",
      "-     \"execution_count\": 75,\n",
      "-     \"metadata\": {},\n",
      "-     \"output_type\": \"execute_result\"\n",
      "-    }\n",
      "-   ],\n",
      "-   \"source\": [\n",
      "-    \"target = dfdf_p[['Parcela', 'idx', 'PROD']]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"target\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 76,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"target.to_csv('target_PROD.csv', index=False)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 77,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [\n",
      "-    {\n",
      "-     \"data\": {\n",
      "-      \"text/html\": [\n",
      "-       \"<div>\\n\",\n",
      "-       \"<style scoped>\\n\",\n",
      "-       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
      "-       \"        vertical-align: middle;\\n\",\n",
      "-       \"    }\\n\",\n",
      "-       \"\\n\",\n",
      "-       \"    .dataframe tbody tr th {\\n\",\n",
      "-       \"        vertical-align: top;\\n\",\n",
      "-       \"    }\\n\",\n",
      "-       \"\\n\",\n",
      "-       \"    .dataframe thead th {\\n\",\n",
      "-       \"        text-align: right;\\n\",\n",
      "-       \"    }\\n\",\n",
      "-       \"</style>\\n\",\n",
      "-       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
      "-       \"  <thead>\\n\",\n",
      "-       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
      "-       \"      <th></th>\\n\",\n",
      "-       \"      <th>Parcela</th>\\n\",\n",
      "-       \"      <th>idx</th>\\n\",\n",
      "-       \"      <th>PROD</th>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"  </thead>\\n\",\n",
      "-       \"  <tbody>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>0</th>\\n\",\n",
      "-       \"      <td>FUEN34-16</td>\\n\",\n",
      "-       \"      <td>3502</td>\\n\",\n",
      "-       \"      <td>31595</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>1</th>\\n\",\n",
      "-       \"      <td>FUEN34-11</td>\\n\",\n",
      "-       \"      <td>3553</td>\\n\",\n",
      "-       \"      <td>3285</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>2</th>\\n\",\n",
      "-       \"      <td>FUEN34-21</td>\\n\",\n",
      "-       \"      <td>3582</td>\\n\",\n",
      "-       \"      <td>2581</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>3</th>\\n\",\n",
      "-       \"      <td>FUEN34-4</td>\\n\",\n",
      "-       \"      <td>3804</td>\\n\",\n",
      "-       \"      <td>13155</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>4</th>\\n\",\n",
      "-       \"      <td>NAVA05-22</td>\\n\",\n",
      "-       \"      <td>3882</td>\\n\",\n",
      "-       \"      <td>2487</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>5</th>\\n\",\n",
      "-       \"      <td>FUEN31-30</td>\\n\",\n",
      "-       \"      <td>3914</td>\\n\",\n",
      "-       \"      <td>0</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>6</th>\\n\",\n",
      "-       \"      <td>NAVA05-24</td>\\n\",\n",
      "-       \"      <td>3942</td>\\n\",\n",
      "-       \"      <td>3843</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>7</th>\\n\",\n",
      "-       \"      <td>FUEN34-15</td>\\n\",\n",
      "-       \"      <td>3998</td>\\n\",\n",
      "-       \"      <td>2537</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>8</th>\\n\",\n",
      "-       \"      <td>FUEN34-5</td>\\n\",\n",
      "-       \"      <td>4000</td>\\n\",\n",
      "-       \"      <td>14912</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>9</th>\\n\",\n",
      "-       \"      <td>NAVA05-3</td>\\n\",\n",
      "-       \"      <td>4053</td>\\n\",\n",
      "-       \"      <td>10374</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>10</th>\\n\",\n",
      "-       \"      <td>FUEN31-5</td>\\n\",\n",
      "-       \"      <td>4156</td>\\n\",\n",
      "-       \"      <td>5482</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>11</th>\\n\",\n",
      "-       \"      <td>FUEN31-6</td>\\n\",\n",
      "-       \"      <td>4189</td>\\n\",\n",
      "-       \"      <td>6076</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>12</th>\\n\",\n",
      "-       \"      <td>FUEN31-14</td>\\n\",\n",
      "-       \"      <td>4196</td>\\n\",\n",
      "-       \"      <td>4489</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>13</th>\\n\",\n",
      "-       \"      <td>FUEN34-24</td>\\n\",\n",
      "-       \"      <td>4245</td>\\n\",\n",
      "-       \"      <td>1235</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>14</th>\\n\",\n",
      "-       \"      <td>FUEN34-0</td>\\n\",\n",
      "-       \"      <td>4261</td>\\n\",\n",
      "-       \"      <td>12240</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>15</th>\\n\",\n",
      "-       \"      <td>FUEN34-1</td>\\n\",\n",
      "-       \"      <td>4262</td>\\n\",\n",
      "-       \"      <td>10599</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>16</th>\\n\",\n",
      "-       \"      <td>FUEN34-1.1</td>\\n\",\n",
      "-       \"      <td>4263</td>\\n\",\n",
      "-       \"      <td>4426</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>17</th>\\n\",\n",
      "-       \"      <td>FUEN34-25</td>\\n\",\n",
      "-       \"      <td>4284</td>\\n\",\n",
      "-       \"      <td>691</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>18</th>\\n\",\n",
      "-       \"      <td>FUEN31-36</td>\\n\",\n",
      "-       \"      <td>4285</td>\\n\",\n",
      "-       \"      <td>5631</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>19</th>\\n\",\n",
      "-       \"      <td>FUEN34-8</td>\\n\",\n",
      "-       \"      <td>4422</td>\\n\",\n",
      "-       \"      <td>2173</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>20</th>\\n\",\n",
      "-       \"      <td>FUEN39-8</td>\\n\",\n",
      "-       \"      <td>4424</td>\\n\",\n",
      "-       \"      <td>1729</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>21</th>\\n\",\n",
      "-       \"      <td>FUEN34-10</td>\\n\",\n",
      "-       \"      <td>4426</td>\\n\",\n",
      "-       \"      <td>12134</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>22</th>\\n\",\n",
      "-       \"      <td>FUEN34-23</td>\\n\",\n",
      "-       \"      <td>4732</td>\\n\",\n",
      "-       \"      <td>3248</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>23</th>\\n\",\n",
      "-       \"      <td>NAVA05-21</td>\\n\",\n",
      "-       \"      <td>4777</td>\\n\",\n",
      "-       \"      <td>4940</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>24</th>\\n\",\n",
      "-       \"      <td>NAVA05-86</td>\\n\",\n",
      "-       \"      <td>5246</td>\\n\",\n",
      "-       \"      <td>4322</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"  </tbody>\\n\",\n",
      "-       \"</table>\\n\",\n",
      "-       \"</div>\"\n",
      "-      ],\n",
      "-      \"text/plain\": [\n",
      "-       \"       Parcela   idx   PROD\\n\",\n",
      "-       \"0    FUEN34-16  3502  31595\\n\",\n",
      "-       \"1    FUEN34-11  3553   3285\\n\",\n",
      "-       \"2    FUEN34-21  3582   2581\\n\",\n",
      "-       \"3     FUEN34-4  3804  13155\\n\",\n",
      "-       \"4    NAVA05-22  3882   2487\\n\",\n",
      "-       \"5    FUEN31-30  3914      0\\n\",\n",
      "-       \"6    NAVA05-24  3942   3843\\n\",\n",
      "-       \"7    FUEN34-15  3998   2537\\n\",\n",
      "-       \"8     FUEN34-5  4000  14912\\n\",\n",
      "-       \"9     NAVA05-3  4053  10374\\n\",\n",
      "-       \"10    FUEN31-5  4156   5482\\n\",\n",
      "-       \"11    FUEN31-6  4189   6076\\n\",\n",
      "-       \"12   FUEN31-14  4196   4489\\n\",\n",
      "-       \"13   FUEN34-24  4245   1235\\n\",\n",
      "-       \"14    FUEN34-0  4261  12240\\n\",\n",
      "-       \"15    FUEN34-1  4262  10599\\n\",\n",
      "-       \"16  FUEN34-1.1  4263   4426\\n\",\n",
      "-       \"17   FUEN34-25  4284    691\\n\",\n",
      "-       \"18   FUEN31-36  4285   5631\\n\",\n",
      "-       \"19    FUEN34-8  4422   2173\\n\",\n",
      "-       \"20    FUEN39-8  4424   1729\\n\",\n",
      "-       \"21   FUEN34-10  4426  12134\\n\",\n",
      "-       \"22   FUEN34-23  4732   3248\\n\",\n",
      "-       \"23   NAVA05-21  4777   4940\\n\",\n",
      "-       \"24   NAVA05-86  5246   4322\"\n",
      "-      ]\n",
      "-     },\n",
      "-     \"execution_count\": 77,\n",
      "-     \"metadata\": {},\n",
      "-     \"output_type\": \"execute_result\"\n",
      "-    }\n",
      "-   ],\n",
      "-   \"source\": [\n",
      "-    \"target = pd.read_csv('target_PROD.csv')\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"target\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 24,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"l = [31595,3285,2581,13155,2487,3843,2537,14912,10374,5482,6076,4489,1235,12240,10599,4426,691,5631,2173,1729,12134,3248,4940,4322,5000]\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 79,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [\n",
      "-    {\n",
      "-     \"data\": {\n",
      "-      \"text/html\": [\n",
      "-       \"<div>\\n\",\n",
      "-       \"<style scoped>\\n\",\n",
      "-       \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
      "-       \"        vertical-align: middle;\\n\",\n",
      "-       \"    }\\n\",\n",
      "-       \"\\n\",\n",
      "-       \"    .dataframe tbody tr th {\\n\",\n",
      "-       \"        vertical-align: top;\\n\",\n",
      "-       \"    }\\n\",\n",
      "-       \"\\n\",\n",
      "-       \"    .dataframe thead th {\\n\",\n",
      "-       \"        text-align: right;\\n\",\n",
      "-       \"    }\\n\",\n",
      "-       \"</style>\\n\",\n",
      "-       \"<table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
      "-       \"  <thead>\\n\",\n",
      "-       \"    <tr style=\\\"text-align: right;\\\">\\n\",\n",
      "-       \"      <th></th>\\n\",\n",
      "-       \"      <th>PROD</th>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"  </thead>\\n\",\n",
      "-       \"  <tbody>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>0</th>\\n\",\n",
      "-       \"      <td>31595</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>1</th>\\n\",\n",
      "-       \"      <td>3285</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>2</th>\\n\",\n",
      "-       \"      <td>2581</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>3</th>\\n\",\n",
      "-       \"      <td>13155</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>4</th>\\n\",\n",
      "-       \"      <td>2487</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>5</th>\\n\",\n",
      "-       \"      <td>0</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>6</th>\\n\",\n",
      "-       \"      <td>3843</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>7</th>\\n\",\n",
      "-       \"      <td>2537</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>8</th>\\n\",\n",
      "-       \"      <td>14912</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>9</th>\\n\",\n",
      "-       \"      <td>10374</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>10</th>\\n\",\n",
      "-       \"      <td>5482</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>11</th>\\n\",\n",
      "-       \"      <td>6076</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>12</th>\\n\",\n",
      "-       \"      <td>4489</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>13</th>\\n\",\n",
      "-       \"      <td>1235</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>14</th>\\n\",\n",
      "-       \"      <td>12240</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>15</th>\\n\",\n",
      "-       \"      <td>10599</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>16</th>\\n\",\n",
      "-       \"      <td>4426</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>17</th>\\n\",\n",
      "-       \"      <td>691</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>18</th>\\n\",\n",
      "-       \"      <td>5631</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>19</th>\\n\",\n",
      "-       \"      <td>2173</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>20</th>\\n\",\n",
      "-       \"      <td>1729</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>21</th>\\n\",\n",
      "-       \"      <td>12134</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>22</th>\\n\",\n",
      "-       \"      <td>3248</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>23</th>\\n\",\n",
      "-       \"      <td>4940</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"    <tr>\\n\",\n",
      "-       \"      <th>24</th>\\n\",\n",
      "-       \"      <td>4322</td>\\n\",\n",
      "-       \"    </tr>\\n\",\n",
      "-       \"  </tbody>\\n\",\n",
      "-       \"</table>\\n\",\n",
      "-       \"</div>\"\n",
      "-      ],\n",
      "-      \"text/plain\": [\n",
      "-       \"     PROD\\n\",\n",
      "-       \"0   31595\\n\",\n",
      "-       \"1    3285\\n\",\n",
      "-       \"2    2581\\n\",\n",
      "-       \"3   13155\\n\",\n",
      "-       \"4    2487\\n\",\n",
      "-       \"5       0\\n\",\n",
      "-       \"6    3843\\n\",\n",
      "-       \"7    2537\\n\",\n",
      "-       \"8   14912\\n\",\n",
      "-       \"9   10374\\n\",\n",
      "-       \"10   5482\\n\",\n",
      "-       \"11   6076\\n\",\n",
      "-       \"12   4489\\n\",\n",
      "-       \"13   1235\\n\",\n",
      "-       \"14  12240\\n\",\n",
      "-       \"15  10599\\n\",\n",
      "-       \"16   4426\\n\",\n",
      "-       \"17    691\\n\",\n",
      "-       \"18   5631\\n\",\n",
      "-       \"19   2173\\n\",\n",
      "-       \"20   1729\\n\",\n",
      "-       \"21  12134\\n\",\n",
      "-       \"22   3248\\n\",\n",
      "-       \"23   4940\\n\",\n",
      "-       \"24   4322\"\n",
      "-      ]\n",
      "-     },\n",
      "-     \"execution_count\": 79,\n",
      "-     \"metadata\": {},\n",
      "-     \"output_type\": \"execute_result\"\n",
      "-    }\n",
      "-   ],\n",
      "-   \"source\": [\n",
      "-    \"l = target[['PROD']]\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"l\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 89,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"from sklearn.preprocessing import LabelEncoder\\n\",\n",
      "-    \"from sklearn.model_selection import train_test_split\\n\",\n",
      "-    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"X = pd.read_csv(r'./preprocess.csv')\\n\",\n",
      "-    \"y = l\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 111,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [],\n",
      "-   \"source\": [\n",
      "-    \"SEED = 42\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"X_train, X_test, y_train, y_test= train_test_split(\\n\",\n",
      "-    \"    \\n\",\n",
      "-    \"    X,y,\\n\",\n",
      "-    \"    train_size   = 0.8,\\n\",\n",
      "-    \"    random_state = SEED,\\n\",\n",
      "-    \")\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 114,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [\n",
      "-    {\n",
      "-     \"name\": \"stdout\",\n",
      "-     \"output_type\": \"stream\",\n",
      "-     \"text\": [\n",
      "-      \"[5334.98095238 4824.98095238 6117.26666667 5806.1        3176.76666667]\\n\"\n",
      "-     ]\n",
      "-    },\n",
      "-    {\n",
      "-     \"name\": \"stderr\",\n",
      "-     \"output_type\": \"stream\",\n",
      "-     \"text\": [\n",
      "-      \"C:\\\\Users\\\\reuni\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_15052\\\\3651426399.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  clf.fit(X_train, y_train)\\n\"\n",
      "-     ]\n",
      "-    }\n",
      "-   ],\n",
      "-   \"source\": [\n",
      "-    \"from sklearn.ensemble import RandomForestRegressor\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"clf = RandomForestRegressor(n_estimators=3, max_depth=2, random_state=SEED)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"clf.fit(X_train, y_train)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"y_pred = clf.predict(X_test)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"print(y_pred)\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 116,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [\n",
      "-    {\n",
      "-     \"name\": \"stdout\",\n",
      "-     \"output_type\": \"stream\",\n",
      "-     \"text\": [\n",
      "-      \"Mean Absolute Error (MAE): 7843.8133333333335\\n\",\n",
      "-      \"Mean Squared Error (MSE): 150029811.71487528\\n\",\n",
      "-      \"Root Mean Squared Error (RMSE): 12248.665711614278\\n\"\n",
      "-     ]\n",
      "-    }\n",
      "-   ],\n",
      "-   \"source\": [\n",
      "-    \"import numpy as np\\n\",\n",
      "-    \"from sklearn import metrics\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, y_pred))\\n\",\n",
      "-    \"print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, y_pred))\\n\",\n",
      "-    \"print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 118,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [\n",
      "-    {\n",
      "-     \"name\": \"stderr\",\n",
      "-     \"output_type\": \"stream\",\n",
      "-     \"text\": [\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  estimator.fit(X_train, y_train, **fit_params)\\n\",\n",
      "-      \"c:\\\\Python38\\\\lib\\\\site-packages\\\\sklearn\\\\model_selection\\\\_search.py:926: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  self.best_estimator_.fit(X, y, **fit_params)\\n\"\n",
      "-     ]\n",
      "-    },\n",
      "-    {\n",
      "-     \"data\": {\n",
      "-      \"text/plain\": [\n",
      "-       \"GridSearchCV(cv=5, estimator=RandomForestRegressor(),\\n\",\n",
      "-       \"             param_grid={'max_depth': [3, 4, 5, 6, 7],\\n\",\n",
      "-       \"                         'max_features': ['sqrt', 'log2'],\\n\",\n",
      "-       \"                         'n_estimators': [5, 10, 25], 'random_state': [42]})\"\n",
      "-      ]\n",
      "-     },\n",
      "-     \"execution_count\": 118,\n",
      "-     \"metadata\": {},\n",
      "-     \"output_type\": \"execute_result\"\n",
      "-    }\n",
      "-   ],\n",
      "-   \"source\": [\n",
      "-    \"from sklearn.model_selection import train_test_split\\n\",\n",
      "-    \"from sklearn.model_selection import cross_val_score\\n\",\n",
      "-    \"from sklearn.model_selection import GridSearchCV\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"## Define Grid \\n\",\n",
      "-    \"grid = { \\n\",\n",
      "-    \"    'n_estimators': [5,10,25],\\n\",\n",
      "-    \"    'max_features': ['sqrt','log2'],\\n\",\n",
      "-    \"    'max_depth' : [3,4,5,6,7],\\n\",\n",
      "-    \"    'random_state' : [SEED]\\n\",\n",
      "-    \"}## show start time\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"CV_rfr = GridSearchCV(estimator=RandomForestRegressor(), param_grid=grid, cv= 5)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"CV_rfr.fit(X_train, y_train)## show end time\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"CV_rfr\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 119,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [\n",
      "-    {\n",
      "-     \"data\": {\n",
      "-      \"text/plain\": [\n",
      "-       \"{'max_depth': 3,\\n\",\n",
      "-       \" 'max_features': 'sqrt',\\n\",\n",
      "-       \" 'n_estimators': 10,\\n\",\n",
      "-       \" 'random_state': 42}\"\n",
      "-      ]\n",
      "-     },\n",
      "-     \"execution_count\": 119,\n",
      "-     \"metadata\": {},\n",
      "-     \"output_type\": \"execute_result\"\n",
      "-    }\n",
      "-   ],\n",
      "-   \"source\": [\n",
      "-    \"CV_rfr.best_params_\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 121,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [\n",
      "-    {\n",
      "-     \"name\": \"stderr\",\n",
      "-     \"output_type\": \"stream\",\n",
      "-     \"text\": [\n",
      "-      \"C:\\\\Users\\\\reuni\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_15052\\\\4087170470.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\\n\",\n",
      "-      \"  rfc1.fit(X_train, y_train)\\n\"\n",
      "-     ]\n",
      "-    },\n",
      "-    {\n",
      "-     \"data\": {\n",
      "-      \"text/plain\": [\n",
      "-       \"RandomForestRegressor(max_depth=3, max_features='sqrt', n_estimators=10,\\n\",\n",
      "-       \"                      random_state=42)\"\n",
      "-      ]\n",
      "-     },\n",
      "-     \"execution_count\": 121,\n",
      "-     \"metadata\": {},\n",
      "-     \"output_type\": \"execute_result\"\n",
      "-    }\n",
      "-   ],\n",
      "-   \"source\": [\n",
      "-    \"rfc1 = RandomForestRegressor(max_depth=3, max_features='sqrt', n_estimators= 10, random_state=42)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"rfc1.fit(X_train, y_train)\\n\",\n",
      "-    \"\\n\"\n",
      "-   ]\n",
      "-  },\n",
      "-  {\n",
      "-   \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 123,\n",
      "-   \"metadata\": {},\n",
      "-   \"outputs\": [\n",
      "-    {\n",
      "-     \"data\": {\n",
      "-      \"text/plain\": [\n",
      "-       \"array([7621.26154762, 5317.7825    , 7980.97738095, 5329.38809524,\\n\",\n",
      "-       \"       8267.41666667])\"\n",
      "-      ]\n",
      "-     },\n",
      "-     \"execution_count\": 123,\n",
      "-     \"metadata\": {},\n",
      "-     \"output_type\": \"execute_result\"\n",
      "-    }\n",
      "-   ],\n",
      "-   \"source\": [\n",
      "-    \"pred=rfc1.predict(X_test)\\n\",\n",
      "-    \"\\n\",\n",
      "-    \"pred\"\n",
      "-   ]\n",
      "-  }\n",
      "- ],\n",
      "- \"metadata\": {\n",
      "-  \"kernelspec\": {\n",
      "-   \"display_name\": \"Python 3\",\n",
      "-   \"language\": \"python\",\n",
      "-   \"name\": \"python3\"\n",
      "-  },\n",
      "-  \"language_info\": {\n",
      "-   \"codemirror_mode\": {\n",
      "-    \"name\": \"ipython\",\n",
      "-    \"version\": 3\n",
      "-   },\n",
      "-   \"file_extension\": \".py\",\n",
      "-   \"mimetype\": \"text/x-python\",\n",
      "-   \"name\": \"python\",\n",
      "-   \"nbconvert_exporter\": \"python\",\n",
      "-   \"pygments_lexer\": \"ipython3\",\n",
      "-   \"version\": \"3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]\"\n",
      "-  },\n",
      "-  \"orig_nbformat\": 4,\n",
      "-  \"vscode\": {\n",
      "-   \"interpreter\": {\n",
      "-    \"hash\": \"9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2\"\n",
      "-   }\n",
      "-  }\n",
      "- },\n",
      "- \"nbformat\": 4,\n",
      "- \"nbformat_minor\": 2\n",
      "-}\n",
      "diff --git a/preprocess.csv b/preprocess.csv\n",
      "deleted file mode 100644\n",
      "index 874e963..0000000\n",
      "--- a/preprocess.csv\n",
      "+++ /dev/null\n",
      "@@ -1,26 +0,0 @@\n",
      "-IDX,mean_ndvi_2022-06-02,mean_ndvi_2022-06-17,mean_ndvi_2022-06-07,mean_ndvi_2022-06-22,mean_ndvi_2022-06-12,mean_ndre_2022-06-02,mean_ndre_2022-06-17,mean_ndre_2022-06-07,mean_ndre_2022-06-12,IDX.1,sum_ndvi_2022-06-02,sum_ndvi_2022-06-17,sum_ndvi_2022-06-07,sum_ndvi_2022-06-22,sum_ndvi_2022-06-12,sum_ndre_2022-06-02,sum_ndre_2022-06-17,sum_ndre_2022-06-07,sum_ndre_2022-06-12\n",
      "-3942,0.0074764119600181,0.2127703350866087,0.1808441304228756,0.0384776496841147,0.219895889258849,-0.0170729657877585,0.1517137726781239,0.1237459981978032,0.1533328888835571,3942,0.5308252491612857,15.106693791149215,12.839933260024171,2.7319131275721493,15.612608137378285,-1.2121805709308555,10.771677860146797,8.785965872044027,10.88663511073256\n",
      "-4053,0.0087089333547304,0.2020584995264399,0.1650456686788255,-0.0195839866930479,0.1988123692947914,-0.0077377533792267,0.1348862907559904,0.1060430193136325,0.1287979661240226,4053,1.6895330708177,39.19934890812934,32.018859723692145,-3.799293418451296,38.569599643189534,-1.5011241555699846,26.167940406662147,20.572345746844704,24.986805428060386\n",
      "-4196,0.2045757792607901,0.1929386520555124,0.1567452417887679,-0.0114480870356029,0.1866423150688147,0.226346905325337,0.1286577624258592,0.0956563953504208,0.1170902026434265,4196,19.639274809035857,18.52211059732919,15.047543211721724,-1.0990163554178816,17.917662246606213,21.72930291123236,12.35114519288249,9.1830139536404,11.240659453768949\n",
      "-4422,-0.001837647991757,0.198716591902953,0.1813427211269491,-0.0028939568066261,0.2154282339398172,-0.0368779941542969,0.132829284745391,0.113878931586843,0.1360886120243491,4422,-0.104745935530154,11.32684573846832,10.336535104236098,-0.164955537977692,12.279409334569582,-2.1020456667949263,7.571269230487287,6.491099100450054,7.7570508853879\n",
      "-4424,-0.0467620928367819,0.2054815228036352,0.1857075772109727,-0.0053765417802805,0.2182370056311604,-0.0942551479805548,0.1451783818583132,0.1245556683893127,0.1482077987585986,4424,-1.7301974349609308,7.602816343734505,6.871180356805993,-0.1989320458703795,8.074769208352937,-3.487440475280528,5.37160012875759,4.608559730404573,5.48368855406815\n",
      "-3882,0.0005918491361654,0.2248652094663545,0.1896263304763206,0.0622455790079654,0.2335147099283531,-0.0181717554083066,0.1592563116713329,0.1295608956156735,0.1597153954878677,3882,0.0331435516252645,12.592451730115853,10.619074506673956,3.485752424446066,13.076823755987776,-1.0176183028651744,8.918353453594642,7.255410154477721,8.944062147320595\n",
      "-4732,0.2562454367678534,0.1777468812217337,0.1411333250735248,-0.0542809091984421,0.1700758344283176,0.2521103585630991,0.1141631686008887,0.0808824621679879,0.1019073532349795,4732,9.224835723642723,6.398887723982414,5.080799702646894,-1.9541127311439168,6.122730039419434,9.07597290827157,4.109874069631996,2.9117686380475645,3.668664716459263\n",
      "-3553,0.0367588233654958,0.1198552052130903,0.116411518588478,-0.0026436970959803,0.1065592903583344,0.0285438680720223,0.0762079476139548,0.0709527027739342,0.0599916891821797,3553,2.756911752412187,8.989140390981772,8.730863894135856,-0.1982772821985268,7.991946776875084,2.140790105401678,5.715596071046612,5.321452708045067,4.499376688663479\n",
      "-3582,0.1156269993239312,0.2160810915555255,0.1763527478857893,-0.0027948583043145,0.2092272416387304,0.0915893019374318,0.1440462280345384,0.1105699366616586,0.1315687306207059,3582,6.590738961464078,12.316622218664955,10.052106629489993,-0.1593069233459267,11.925952773407634,5.220590210433612,8.210634997968693,6.3024863897145424,7.499417645380239\n",
      "-3914,0.1725584709610202,0.31594087295828,0.2916764013118253,-0.0286903335370747,0.3645014865397951,0.1278668869317516,0.2102045460394906,0.2002812750448416,0.2433468893843765,3914,22.26004275397161,40.75637261161812,37.626255769225466,-3.701053026282645,47.020691763633565,16.494828414195958,27.11638643909429,25.83628448078457,31.39174873058457\n",
      "-3998,-0.0486202150538201,0.2271254970774861,0.2037151570546254,0.0113233313232644,0.2536537816786206,-0.0827527011734857,0.1514828089937198,0.1327443902774966,0.163391185620226,3998,-2.819972473121568,13.173278830494194,11.815479109168274,0.6567532167493401,14.711919337359996,-4.799656668062171,8.786002921635752,7.699174636094806,9.47668876597311\n",
      "-4156,-0.0225821375679074,0.2060104978023064,0.1776844864991618,0.016386303826855,0.2157059898192155,-0.059363197518252,0.1460674413507204,0.1222188411247505,0.1470378093436392,4156,-2.5066172700377227,22.867165256056012,19.722978001406965,1.8188797247809048,23.94336486993292,-6.589314924525973,16.213485989929968,13.566291364847306,16.321196837143958\n",
      "-4245,0.0023010493171388,0.1928255444572992,0.1849682632659965,0.0155281106495753,0.2131512015744318,-0.0390603244379114,0.1392278497002363,0.1344424764647228,0.1535825905203751,4245,0.0989451206369703,8.291498411663868,7.953635320437851,0.6677087579317379,9.165501667700571,-1.6795939508301907,5.986797537110164,5.78102648798308,6.604051392376132\n",
      "-4261,0.0001725277077452,0.1814643467980725,0.1644885149678532,0.0123272078779433,0.1828431890035062,-0.0143161076904791,0.1175212174800594,0.1064943238931965,0.1152343504217193,4261,0.0358857632110036,37.74458413399908,34.21361111331348,2.5640592386122094,38.0313833127293,-2.977750399619666,24.44441323585236,22.150819369784884,23.96874488771763\n",
      "-4263,0.1323246562004196,0.2218479000936428,0.2149171115771083,0.0326382998934491,0.2618118832222543,0.1487610008076806,0.1397778918424999,0.1435285260708006,0.1704884689453216,4263,13.232465620041967,22.18479000936428,21.49171115771084,3.263829989344912,26.18118832222543,14.876100080768062,13.977789184249998,14.35285260708006,17.048846894532165\n",
      "-5246,0.0059924559028977,0.2141391042784046,0.2026256853256827,-0.0124712273116344,0.2322153711074927,-0.0057172755698618,0.1488327594134763,0.1379184095056473,0.1556730717378338,5246,0.4973738399405125,17.77354565510759,16.817931882031672,-1.0351118668656598,19.2738758019219,-0.4745338722985299,12.353119031318531,11.447227988968727,12.920864954240209\n",
      "-3502,0.1851889438193691,0.2181301689560501,0.187833795358135,-0.0318541267182072,0.2290860896423927,0.2243255068119433,0.1437631674414004,0.1203765499562081,0.1473285275093991,3502,46.29723595484229,54.53254223901254,46.958448839533744,-7.963531679551805,57.2715224105982,56.08137670298583,35.94079186035012,30.09413748905203,36.83213187734979\n",
      "-4285,0.1507574438860988,0.2566063297342592,0.2137275430846321,-0.0080459095882469,0.2620160846129526,0.1690325800614144,0.1768173051306308,0.1362185099103667,0.169355531184839,4285,18.99543792964845,32.33239754651666,26.92967042866365,-1.0137846081191124,33.01402666123204,21.298105087738215,22.27898044645949,17.16353224870621,21.338796929289717\n",
      "-4777,0.0260461815210129,0.2256856742289642,0.1912748446669856,0.0431576574815238,0.2440646823067591,-0.0043803022764667,0.1513579336109525,0.1265955114713985,0.1611707392284474,4777,2.682756696664336,23.24562444558332,19.70130900069952,4.445238720596952,25.138662277596197,-0.4511711344760732,15.589867161928115,13.03933768155405,16.600586140530087\n",
      "-3804,0.0090155545454931,0.2249113677422857,0.210743928997123,0.0188203924011587,0.2334723610313366,-0.012431895084969,0.1520788743797985,0.1382932256854848,0.1524450931599094,3804,2.17274864546385,54.20363962589086,50.78928688830665,4.535714568679249,56.26683900855214,-2.996086715477534,36.65100872553145,33.32866739020184,36.73926745153817\n",
      "-4000,0.1549545478407438,0.226453705167114,0.1990423448649953,-0.0026035611941514,0.2368062359849951,0.1220386780840442,0.1504950905829815,0.1281949290176416,0.1534742496283907,4000,40.13322789075265,58.65150963828255,51.5519673200338,-0.6743223492852157,61.332815120113736,31.608017623767463,38.978228460992206,33.202486615569185,39.7498306537532\n",
      "-4189,-0.0157297142996901,0.2195371700455565,0.1906269267466007,0.0123966950023547,0.2338760124369117,-0.0443621258004192,0.1540939016473098,0.1225263352270105,0.154364085760177,4189,-2.0291331446600345,28.320294935876788,24.590873550311493,1.5991736553037683,30.170005604361613,-5.722714228254083,19.87811331250297,15.805897244284369,19.912967063062844\n",
      "-4262,0.1883804343832237,0.249993735829132,0.2249656313900213,0.0348828436719208,0.2787068496062342,0.2254958615870058,0.1682600255298347,0.1553265872609329,0.1909229310532905,4262,36.73418470472862,48.74877848668075,43.86829812105417,6.8021545160245696,54.34783567321567,43.97169300946613,32.81070497831778,30.28868451588193,37.22997155539165\n",
      "-4284,0.0285243165496032,0.2298163968154127,0.1987223925190127,0.0111666974603294,0.2369508562206129,0.0046920734533612,0.1660903787116705,0.1360528694831915,0.1622692083076899,4284,0.5704863309920657,4.596327936308255,3.974447850380255,0.2233339492065899,4.739017124412259,0.0938414690672257,3.3218075742334103,2.721057389663831,3.2453841661537988\n",
      "-4426,-0.0097490571099672,0.2248255458985654,0.2133936044395444,0.0006529451694381,0.2515282720085002,-0.049095118365384,0.1502989235217334,0.1406698851747752,0.1614381810895613,4426,-2.1447925641928007,49.4616200976844,46.946592976699776,0.1436479372763913,55.33621984187006,-10.8009260403845,33.06576317478135,30.94737473845056,35.516399839703496\n",
      "diff --git a/producciones.xlsx b/producciones.xlsx\n",
      "deleted file mode 100644\n",
      "index a3c223f..0000000\n",
      "Binary files a/producciones.xlsx and /dev/null differ\n",
      "diff --git a/random_forest.ipynb b/random_forest.ipynb\n",
      "index b25c9ae..cabf5c9 100644\n",
      "--- a/random_forest.ipynb\n",
      "+++ b/random_forest.ipynb\n",
      "@@ -2,7 +2,7 @@\n",
      "  \"cells\": [\n",
      "   {\n",
      "    \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 52,\n",
      "+   \"execution_count\": 71,\n",
      "    \"metadata\": {},\n",
      "    \"outputs\": [],\n",
      "    \"source\": [\n",
      "@@ -13,7 +13,7 @@\n",
      "   },\n",
      "   {\n",
      "    \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 53,\n",
      "+   \"execution_count\": 72,\n",
      "    \"metadata\": {},\n",
      "    \"outputs\": [\n",
      "     {\n",
      "@@ -223,24 +223,24 @@\n",
      "        \"4             5.371600             4.608560             5.483689  \"\n",
      "       ]\n",
      "      },\n",
      "-     \"execution_count\": 53,\n",
      "+     \"execution_count\": 72,\n",
      "      \"metadata\": {},\n",
      "      \"output_type\": \"execute_result\"\n",
      "     }\n",
      "    ],\n",
      "    \"source\": [\n",
      "-    \"df = pd.read_csv(r'./preprocess.csv')\\n\",\n",
      "+    \"df = pd.read_csv(r'./data/preprocess.csv')\\n\",\n",
      "     \"\\n\",\n",
      "     \"df.head()\"\n",
      "    ]\n",
      "   },\n",
      "   {\n",
      "    \"cell_type\": \"code\",\n",
      "-   \"execution_count\": 54,\n",
      "+   \"execution_count\": 73,\n",
      "    \"metadata\": {},\n",
      "    \"outputs\": [],\n",
      "    \"source\": [\n",
      "-    \"target = pd.read_csv('target_PROD.csv')\"\n",
      "+    \"target = pd.read_csv('./data/target_PROD.csv')\"\n",
      "    ]\n",
      "   },\n",
      "   {\n",
      "diff --git a/target_PROD.csv b/target_PROD.csv\n",
      "deleted file mode 100644\n",
      "index dd018b7..0000000\n",
      "--- a/target_PROD.csv\n",
      "+++ /dev/null\n",
      "@@ -1,26 +0,0 @@\n",
      "-Parcela,idx,PROD\n",
      "-FUEN34-16,3502,31595\n",
      "-FUEN34-11,3553,3285\n",
      "-FUEN34-21,3582,2581\n",
      "-FUEN34-4,3804,13155\n",
      "-NAVA05-22,3882,2487\n",
      "-FUEN31-30,3914,0\n",
      "-NAVA05-24,3942,3843\n",
      "-FUEN34-15,3998,2537\n",
      "-FUEN34-5,4000,14912\n",
      "-NAVA05-3,4053,10374\n",
      "-FUEN31-5,4156,5482\n",
      "-FUEN31-6,4189,6076\n",
      "-FUEN31-14,4196,4489\n",
      "-FUEN34-24,4245,1235\n",
      "-FUEN34-0,4261,12240\n",
      "-FUEN34-1,4262,10599\n",
      "-FUEN34-1.1,4263,4426\n",
      "-FUEN34-25,4284,691\n",
      "-FUEN31-36,4285,5631\n",
      "-FUEN34-8,4422,2173\n",
      "-FUEN39-8,4424,1729\n",
      "-FUEN34-10,4426,12134\n",
      "-FUEN34-23,4732,3248\n",
      "-NAVA05-21,4777,4940\n",
      "-NAVA05-86,5246,4322\n",
      "diff --git a/test.py b/test.py\n",
      "deleted file mode 100644\n",
      "index 951391c..0000000\n",
      "--- a/test.py\n",
      "+++ /dev/null\n",
      "@@ -1,27 +0,0 @@\n",
      "-prod = [\n",
      "-    31595,\n",
      "-    3285,\n",
      "-    2581,\n",
      "-    13155,\n",
      "-    2487,\n",
      "-    3843,\n",
      "-    2537,\n",
      "-    14912,\n",
      "-    10374,\n",
      "-    5482,\n",
      "-    6076,\n",
      "-    4489,\n",
      "-    1235,\n",
      "-    12240,\n",
      "-    10599,\n",
      "-    4426,\n",
      "-    691,\n",
      "-    5631,\n",
      "-    2173,\n",
      "-    1729,\n",
      "-    12134\n",
      "-    3248,\n",
      "-    4940,\n",
      "-    4322,\n",
      "-    0\n",
      "-]\n",
      "\\ No newline at end of file\n",
      "alesteba@unirioja.com\n",
      "alesteba\n",
      "master\n",
      "Remotes:\n",
      "- origin https://github.com/alesteba/tfg_models.git\n",
      "Remote name: origin\n",
      "Remote URL: https://github.com/alesteba/tfg_models.git\n"
     ]
    },
    {
     "ename": "GitCommandError",
     "evalue": "Cmd('git') failed due to: exit code(128)\n  cmdline: git push --porcelain origin\n  stderr: 'fatal: The current branch master has no upstream branch.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGitCommandError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32me:\\UR\\TFG\\data\\git_auto.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/UR/TFG/data/git_auto.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m exec(\u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mgitpython.py\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mread())\n",
      "File \u001b[1;32m<string>:98\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\git\\remote.py:952\u001b[0m, in \u001b[0;36mRemote.push\u001b[1;34m(self, refspec, progress, kill_after_timeout, **kwargs)\u001b[0m\n\u001b[0;32m    947\u001b[0m kwargs \u001b[39m=\u001b[39m add_progress(kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepo\u001b[39m.\u001b[39mgit, progress)\n\u001b[0;32m    948\u001b[0m proc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepo\u001b[39m.\u001b[39mgit\u001b[39m.\u001b[39mpush(\u001b[39mself\u001b[39m, refspec, porcelain\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, as_process\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    949\u001b[0m                           universal_newlines\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    950\u001b[0m                           kill_after_timeout\u001b[39m=\u001b[39mkill_after_timeout,\n\u001b[0;32m    951\u001b[0m                           \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 952\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_push_info(proc, progress,\n\u001b[0;32m    953\u001b[0m                            kill_after_timeout\u001b[39m=\u001b[39;49mkill_after_timeout)\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\git\\remote.py:814\u001b[0m, in \u001b[0;36mRemote._get_push_info\u001b[1;34m(self, proc, progress, kill_after_timeout)\u001b[0m\n\u001b[0;32m    812\u001b[0m stderr_text \u001b[39m=\u001b[39m progress\u001b[39m.\u001b[39merror_lines \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(progress\u001b[39m.\u001b[39merror_lines) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    813\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 814\u001b[0m     proc\u001b[39m.\u001b[39;49mwait(stderr\u001b[39m=\u001b[39;49mstderr_text)\n\u001b[0;32m    815\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    816\u001b[0m     \u001b[39m# This is different than fetch (which fails if there is any std_err\u001b[39;00m\n\u001b[0;32m    817\u001b[0m     \u001b[39m# even if there is an output)\u001b[39;00m\n\u001b[0;32m    818\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m output:\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\git\\cmd.py:502\u001b[0m, in \u001b[0;36mGit.AutoInterrupt.wait\u001b[1;34m(self, stderr)\u001b[0m\n\u001b[0;32m    500\u001b[0m     errstr \u001b[39m=\u001b[39m read_all_from_possibly_closed_stream(p_stderr)\n\u001b[0;32m    501\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mAutoInterrupt wait stderr: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (errstr,))\n\u001b[1;32m--> 502\u001b[0m     \u001b[39mraise\u001b[39;00m GitCommandError(remove_password_if_present(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs), status, errstr)\n\u001b[0;32m    503\u001b[0m \u001b[39mreturn\u001b[39;00m status\n",
      "\u001b[1;31mGitCommandError\u001b[0m: Cmd('git') failed due to: exit code(128)\n  cmdline: git push --porcelain origin\n  stderr: 'fatal: The current branch master has no upstream branch.'"
     ]
    }
   ],
   "source": [
    "exec(open(\"gitpython.py\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
